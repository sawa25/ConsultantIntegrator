[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n!pip install -q tiktoken==0.7.0 openai langchain langchain-openai langchain-text-splitters >/dev/null\n\n!pip install langchain_community >/dev/null\n\n!pip install --upgrade --quiet  langchain-qdrant >/dev/null\nimport openai\n\nfrom openai import OpenAI\n\nimport tiktoken\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n\nfrom langchain.docstore.document import Document\n\nfrom langchain_openai import OpenAIEmbeddings\n\nimport os\n\nfrom google.colab import drive\n\ndrive.mount('/content/drive', force_remount=True)\n\nimport re\n\nimport json\n\n\n\nfrom langchain_qdrant import Qdrant\nfrom google.colab import userdata\n\nkey = userdata.get('OpenAI') # \u0437\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0441\u0435\u043a\u0440\u0435\u0442\u0430 \u043a\u043e\u043b\u0430\u0431\u0430 \u043d\u0430 \u0441\u0432\u043e\u0435 (\u0434\u043b\u044f \u043a\u043b\u044e\u0447\u0430 \u041e\u043f\u0435\u043d\u042d\u0439\u0410\u0439)\n\nos.environ[\"OPENAI_API_KEY\"] = key\n\nclient = OpenAI()\nMODEL = 'gpt-3.5-turbo-0125'\ngoogle_folder_path = \"/content/drive/MyDrive/Neural_University/GPT_Practic/24-07-09\"\n\nf_name = '\u0418\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b'\n\nbd_full_name = os.path.join(google_folder_path, f'{f_name}.txt')\n\nwith open(bd_full_name, 'r') as f:\n\n    markdawn = f.read()\n# @title \u0424\u0443\u043d\u043a\u0446\u0438\u0438\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u0442\u043e\u043a\u0435\u043d\u043e\u0432\n\ndef num_tokens(str_, model=MODEL):\n\n    try:\n\n        encoding = tiktoken.encoding_for_model(model)\n\n    except KeyError:\n\n        encoding = tiktoken.get_encoding('cl100k_base')\n\n    num_tokens = len(encoding.encode(str_))\n\n    return num_tokens\n\n\n\n#@title \u0420\u0430\u0437\u0431\u043e\u0440 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 (\u0444\u0443\u043d\u043a\u0446\u0438\u044f)\n\ndef parse_text(text):\n\n    # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u0435\u0442 \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0447\u0430\u043d\u043a\u0438 \u043f\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044e '---...---'\n\n    # \u0422\u0435\u043a\u0441\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u0438 \u0441 \u043c\u0435\u0442\u0430\u0434\u0430\u0442\u043e\u0439 - \u0438\u0445 \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 json\n\n    # \u0415\u0441\u0442\u044c \u043d\u044e\u0430\u043d\u0441\u044b \u043f\u043e \u0437\u0430\u043c\u0435\u043d\u0435 \u043e\u0434\u0438\u043d\u0430\u0440\u043d\u044b\u0445 \u043a\u0430\u0432\u044b\u0447\u0435\u043a \u043d\u0430 \u0434\u0432\u043e\u0439\u043d\u044b\u0435 \u0432 \u043a\u043b\u044e\u0447\u0430\u0445 \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 \u0434\u043b\u044f json-\u0441\u043b\u043e\u0432\u0430\u0440\u0435\u0439\n\n    # \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u043d\u043d\u044b\u0435 \u0447\u0430\u043d\u043a\u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043b\u0430\u043d\u0433\u0447\u0435\u0439\u043d\n\n    #  \u0432 \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u043e\u0432 \u0441 H1 \u043d\u0430 group \u0438 \u0442.\u0434.\n\n\n\n    chunks = text.split('----------------------------------------------------------------')\n\n    source_chunks = []\n\n    # \u0420\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430 JSON \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0447\u0430\u043d\u043a\u0430\n\n    json_pattern = re.compile(r\"\\{.*?\\}\", re.DOTALL)\n\n    for chunk in chunks:\n\n        chunk = chunk.strip()\n\n        if not chunk:\n\n            continue\n\n        # \u041f\u043e\u0438\u0441\u043a \u0441\u043b\u043e\u0432\u0430\u0440\u044f\n\n        json_match = json_pattern.search(chunk)\n\n        if json_match:\n\n            json_str = json_match.group()\n\n            # \u0417\u0430\u043c\u0435\u043d\u0438\u043c \u0434\u0432\u043e\u0439\u043d\u044b\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043d\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u043c\u0430\u0440\u043a\u0435\u0440\n\n            json_str = json_str.replace('\"', 'TEMP_DOUBLE_QUOTE')\n\n            # \u0417\u0430\u043c\u0435\u043d\u0438\u043c \u043e\u0434\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043d\u0430 \u0434\u0432\u043e\u0439\u043d\u044b\u0435 \u0431\u0435\u0437 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0442\u0430\u043a json \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n\n            json_str = json_str.replace(\"'\", '\"')\n\n\n\n            # \u0412\u0435\u0440\u043d\u0435\u043c \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0435 \u0434\u0432\u043e\u0439\u043d\u044b\u0435 \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u0435\u0439 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0441 \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c (\u0434\u043b\u044f json)\n\n            json_str = json_str.replace('TEMP_DOUBLE_QUOTE', '\\\\\"')\n\n            metadata = {}\n\n            try:\n\n                json_dict = json.loads(json_str)\n\n                if 'H1' in json_dict:\n\n                    metadata['group'] = json_dict['H1']\n\n                if 'H2' in json_dict:\n\n                    metadata['subgroup'] = json_dict['H2']\n\n                for i in range(3, 7):\n\n                    if f'H{i}' in json_dict:\n\n                        if json_dict[f'H{i}'] == '\u0422\u043e\u0432\u0430\u0440\u044b':\n\n                            metadata['item'] = json_dict[f'H{i}']\n\n                        else:\n\n                            metadata[f'H{i}'] = json_dict[f'H{i}']\n\n\n\n            except json.JSONDecodeError as e:\n\n                print(f\"\u041e\u0448\u0438\u0431\u043a\u0430 \u0434\u0435\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f JSON: {e}\")\n\n                print(f\"\u0421\u0442\u0440\u043e\u043a\u0430 JSON: {json_str}\")\n\n                continue\n\n\n\n            # \u0423\u0431\u0435\u0440\u0435\u043c JSON \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u043d\u0430\u0447\u0430\u043b\u0430 \u0447\u0430\u043d\u043a\u0430\n\n            page_content = chunk[json_match.end():].strip()\n\n\n\n            # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a\n\n            source_chunks.append(Document(page_content = page_content, metadata = metadata))\n\n\n\n    return source_chunks\n\n\n\n#@title \u0421\u043f\u043b\u0438\u0442\u0442\u0435\u0440 (\u0444\u0443\u043d\u043a\u0446\u0438\u044f)\n\ndef split_doc( text_, max_tokens=100, markdown_max_level=4):\n\n  # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442 text_ \u043d\u0430 \u0447\u0430\u043d\u043a\u0438 \u0434\u043b\u0438\u043d\u044b max_tokens,\n\n  # \u0415\u0441\u043b\u0438 \u0442\u0435\u043a\u0441\u0442 \u043c\u0435\u043d\u044c\u0448\u0435  max_tokens, \u0442\u043e \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043a\u0430\u043a \u0435\u0441\u0442\u044c,\n\n  # \u0438\u043d\u0430\u0447\u0435 \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442 MarkdownHeaderTextSplitter \u0434\u043e \u0443\u0440\u043e\u0432\u043d\u044f markdown_max_level,\n\n  # \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0447\u0430\u043d\u043a\u0438 (\u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 max_tokens) \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442 RecursiveCharacterTextSplitter,\n\n  # \u0432 \u0442\u0435\u043b\u043e \u0447\u0430\u043d\u043a\u043e\u0432 \u0434\u0443\u0431\u043b\u0438\u0440\u0443\u0435\u0442 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 \u0432 \u0438\u0445 \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0438\n\n  # \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043b\u0430\u043d\u0433\u0447\u0435\u0439\u043d:\n\n  #   \u0432 page_content \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0447\u0430\u043d\u043a\u0438,\n\n  #   \u0432 meta_data - \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 \u043c\u0430\u0440\u043a\u0434\u0430\u0443\u043d, \u0438\u043c\u044f \u0444\u0430\u0439\u043b\u0430\n\n\n\n    chunk_list = []\n\n    #headers_to_split_on = [(f\"{'#' * i}\", f\"H{i}\") for i in range(1, markdown_max_level+1)]\n\n    headers_to_split_on = [\n\n        ('#','group'),\n\n        ('##','subgroup'),\n\n        ('###','subsubgroup'),\n\n        ('####','items')\n\n    ]\n\n    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n\n    chunks = markdown_splitter.split_text(text_)\n\n\n\n    for chunk in chunks:\n\n            headers = ''\n\n            ch_txt = chunk.page_content\n\n\n\n\n\n            for header_key in list(chunk.metadata):\n\n                headers += f'\\n{header_key}: {chunk.metadata[header_key]}'\n\n            pcont = f'{headers}\\n{ch_txt}'\n\n            chunk_len = num_tokens(pcont)\n\n\n\n            if chunk_len > max_tokens:\n\n                r_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\"],\n\n                                                            chunk_size = max_tokens - num_tokens(headers),\n\n                                                            chunk_overlap = 0,\n\n                                                            length_function = lambda x: num_tokens(x))\n\n                parts = r_splitter.split_text(pcont)\n\n                for k, part in enumerate(parts):\n\n                    if k == 0: # \u0432 \u043f\u0435\u0440\u0432\u043e\u043c \u0447\u0430\u043d\u043a\u0435 \u0435\u0441\u0442\u044c \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 (\u043e\u0442 \u043c\u0430\u0440\u043a\u0434\u0430\u0443\u043d-\u0447\u0430\u043d\u043a\u0430)\n\n                        ppcont = part\n\n                    else:\n\n                        ppcont = f'\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0435\u043d\u0438\u0435 {k}{headers}\\n{part}'\n\n                    metadata = chunk.metadata\n\n                    metadata['tokens'] =  num_tokens(ppcont)\n\n                    metadata['part_num'] =  k+1\n\n                    chunk_list.append(Document(page_content=ppcont, metadata=metadata))\n\n            else:\n\n                metadata = chunk.metadata\n\n                metadata['tokens'] =  chunk_len\n\n                chunk_list.append(Document(page_content=pcont, metadata=metadata))\n\n\n\n    return chunk_list\nparsed_chunks = parse_text(markdawn)\n\nprint(len(parsed_chunks))\n# \u0412\u044b\u0432\u043e\u0434 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n\nfor chunk in range(4,7):\n\n    print(f'\u0427\u0430\u043d\u043a {chunk}: {parsed_chunks[chunk].metadata}\\n{parsed_chunks[chunk].page_content}')\n\n    print('______________')\nimport matplotlib.pyplot as plt\n\n# \u041f\u043e\u0434\u0441\u0447\u0435\u0442 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u0430 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n\nfragment_token_counts = [num_tokens(fragment.page_content, \"cl100k_base\") for fragment in parsed_chunks]\n\nplt.hist(fragment_token_counts, bins=50, alpha=0.5, label='Fragments')\n\nplt.title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d \u0447\u0430\u043d\u043a\u043e\u0432 \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445')\n\nplt.xlabel('Token Count')\n\nplt.ylabel('Frequency')\n\nplt.show()\n#@title \u0421 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0435\u0439\n\nembeddings = OpenAIEmbeddings()\nqdrant = Qdrant.from_documents(\n\n    parsed_chunks,\n\n    embeddings,\n\n    location=\":memory:\",  # Local mode with in-memory storage only\n\n    collection_name=\"\u0418\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b\",\n\n)\nquery = \"\u041c\u0443\u0441\u043e\u0440\"\n\nfound_docs = qdrant.similarity_search(query, k=3)\nfor doc in found_docs:\n\n  print('__________')\n\n  print(doc.page_content)\n\n# \u0411\u0443\u0434\u0435\u043c \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u0442\u044c\n\nfrom qdrant_client.http import models as rest\n\n\n\nfilter_condition = rest.FieldCondition(\n\n    key='metadata.item',   # \u0437\u0434\u0435\u0441\u044c \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043a\u043b\u044e\u0447 \u0432 \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u043e \u0438 \u0441\u0430\u043c\u0438 \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0435 (\u043a\u0430\u043a \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0442.\u043a. \u0435\u0441\u0442\u044c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b)\n\n    match=rest.MatchValue(value='\u0422\u043e\u0432\u0430\u0440\u044b')\n\n)\n\n\n\nquery_filter = rest.Filter(must=[filter_condition])\n\n\n\nfound_docs = qdrant.similarity_search(query, k=3, filter=query_filter)\n\nlen(found_docs)\nfor doc in found_docs:\n\n  print('__________')\n\n  if doc: print(doc.page_content)\noutput = qdrant.similarity_search(query, k=3, filter={\"metadata\": {\"item\": '\u0422\u043e\u0432\u0430\u0440\u044b'}})\nfor doc in found_docs:\n\n  print('__________')\n\n  if doc: print(doc.page_content)\n\nhttps://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/tests/integration_tests/test_similarity_search.py\n\n\n\n\n    \"\"\"Test end to end construction and search.\"\"\"\n\n    texts = [\"foo\", \"bar\", \"baz\"]\n\n    metadatas = [\n\n        {\"page\": i, \"metadata\": {\"page\": i + 1, \"pages\": [i + 2, -1]}}\n\n        for i in range(len(texts))\n\n    ]\n\n    docsearch = Qdrant.from_texts(\n\n        texts,\n\n        ConsistentFakeEmbeddings(),\n\n        metadatas=metadatas,\n\n        location=\":memory:\",\n\n        batch_size=batch_size,\n\n        vector_name=vector_name,\n\n    )\n\n\n\n    output = docsearch.similarity_search(\n\n        \"foo\", k=1, filter={\"page\": 1, \"metadata\": {\"page\": 2, \"pages\": [3]}}\n\n    )\n\n\n\n    assert_documents_equals(\n\n        actual=output,\n\n        expected=[\n\n            Document(\n\n                page_content=\"bar\",\n\n                metadata={\"page\": 1, \"metadata\": {\"page\": 2, \"pages\": [3, -1]}},\n\n            )\n\n        ],\n\n    )", "metadata": {"subid": 0, "total": 1, "source": "https://colab.research.google.com/drive/1N1t5yPSnoLhKMvZj4wXNSPOypzdsNHl_?usp=drive_link"}}}]