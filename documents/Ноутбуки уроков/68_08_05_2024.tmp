[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n\nhttps://docs.google.com/document/d/1yOq8gjyuYOFGPPWoXOfz8QOPTaF8_2rA/edit?usp=sharing&ouid=118252777260935189446&rtpof=true&sd=true\n\n\n\nhttps://drive.google.com/file/d/1kBG0RkVq825Ev3t5jNllQlDMevhjOTq9/view?usp=sharing\n#\u0412\u0430\u0440\u0438\u0430\u043d\u0442 1\n\n##\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 Word \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0430 Docx2txtLoader (\u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 langchain)\n\n\n\nhttps://python.langchain.com/docs/integrations/document_loaders/microsoft_word/\n!pip install --upgrade --quiet  docx2txt >/dev/null\n\n!pip install langchain_community langchain >/dev/null\nfrom google.colab import drive\n\nfrom langchain_community.document_loaders import Docx2txtLoader\n\n\n\ndrive.mount('/content/drive', force_remount=True)\n#doc_path = \"Computrols.docx\" # \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 (\u0444\u0430\u0439\u043b) \u0432 \u0441\u0440\u0435\u0434\u0443 \u043a\u043e\u043b\u0430\u0431\u0430 \u0438 \u043d\u0435 \u043c\u043e\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0443\u0433\u043b \u0434\u0438\u0441\u043a\n\ndoc_path = \"/content/drive/MyDrive/Neural_University/GPT_Practic/2024_05_08/Computrols.docx\"\n\n\n\nloader = Docx2txtLoader(doc_path)\n\ndata = loader.load()\n\nprint(len(data))\n\nprint(type(data))\n\nprint(type(data[0]))\ndata[0].metadata\n\n# \u0412\u0430\u0440\u0438\u0430\u043d\u0442 2:\n\n## \u041f\u0430\u0440\u0441\u0438\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 Word \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e python-docx\n\n\n\nhttps://python-docx.readthedocs.io/en/latest/\n!pip install python-docx >/dev/null\nimport os\n\nfrom docx import Document\nfrom langchain.docstore.document import Document as doc_lang\n#@title \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0433\u043e \u0434\u043e\u043a-\u0444\u0430\u0439\u043b\u0430 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u0440\u0430\u0437\u0434\u0435\u043b\u043e\u0432 (\u0440\u0430\u0437\u0434\u0435\u043b - \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0438 \u0410\u0431\u0437\u0430\u0446\u0435\u0432 \u0440\u0430\u0437\u0434\u0435\u043b\u0430)\n\ndef read_word_file(file_path):\n\n    doc = Document(file_path)\n\n    chapters = []\n\n    current_chapter = None\n\n\n\n    for paragraph in doc.paragraphs:\n\n        if paragraph.style.name.startswith('Heading'):\n\n            if current_chapter:\n\n                chapters.append(current_chapter)\n\n            current_chapter = []\n\n        if current_chapter is not None:\n\n            current_chapter.append(paragraph.text)\n\n\n\n    if current_chapter:\n\n        chapters.append(current_chapter)\n\n\n\n    return chapters\nchapters = read_word_file(doc_path)\nlen(chapters)\nchunks = []\n\nfor chapter in chapters:\n\n  content = '\\n'.join(chapter)\n\n  chunks.append(doc_lang(page_content = content, metadata={'chapter':chapter[0]}))\n\n\nchunks[31]\n!pip install -q tiktoken==0.5.2  >/dev/null\n# \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\n\nimport tiktoken\n\nimport matplotlib.pyplot as plt\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n\n      \"\"\"\u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u0441\u0442\u0440\u043e\u043a\u0435\"\"\"\n\n      encoding = tiktoken.get_encoding(encoding_name)\n\n      num_tokens = len(encoding.encode(string))\n\n      return num_tokens\nfragment_token_counts = [num_tokens_from_string(chunk.page_content, \"cl100k_base\") for chunk in chunks]\n\nplt.hist(fragment_token_counts, bins=100, alpha=0.5, label='Chunks')\n\nplt.title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d \u0447\u0430\u043d\u043a\u043e\u0432 \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445')\n\nplt.xlabel('Token Count')\n\nplt.ylabel('Frequency')\n\nplt.show()", "metadata": {"subid": 0, "total": 1, "source": "https://colab.research.google.com/drive/1cNqqEgSsbLcXsYxT-V2weWsF4sAhofnp"}}}]