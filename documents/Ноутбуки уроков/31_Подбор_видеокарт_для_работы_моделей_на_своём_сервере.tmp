[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "", "metadata": {"subid": 0, "total": 2, "source": "https://colab.research.google.com/drive/1vDISDnbOB89WXz41bFhWfYGASl-QtI1g?usp=sharing"}}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n# \u041f\u043e\u0434\u0431\u043e\u0440 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 (LLM) \u043d\u0430 \u0441\u0432\u043e\u0451\u043c \u0441\u0435\u0440\u0432\u0435\u0440\u0435 (\u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0435).\n## \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435.\n\u0412\u044b\u0434\u0435\u043b\u0438\u043c \u0437\u0430\u0434\u0430\u0447\u0438 \u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 LLM:\n\n  * \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 pre-trained \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u043d\u0443\u043b\u044f\n\n  * pre-train \u0443\u0436\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u043b\u0430\u0441\u0442\u0438\n\n  * fine-tuning\n\n  * inference \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u043d\u0435\u0435 pre-trained \u043c\u043e\u0434\u0435\u043b\u0438.\n\n\n\n\n\n\n\n\u0412 \u0440\u0430\u0437\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f pre-trained, \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0435\u0441\u044f\u0442\u043a\u0438 \u0438 \u0434\u0430\u0436\u0435 \u0441\u043e\u0442\u043d\u0438 \u0434\u043e\u0440\u043e\u0433\u0438\u0445 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442. \u0412 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043e\u0442 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 128 GPU A100 80 \u0413\u0411 \u043e\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0434\u043d\u0435\u0439 \u0434\u043e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043c\u0435\u0441\u044f\u0446\u0435\u0432.\n\n\n\n\u0414\u0430\u0436\u0435 fine-tuning \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 52 \u0442\u044b\u0441\u044f\u0447\u0430\u0445 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0434\u0438\u0430\u043b\u043e\u0433\u043e\u0432, \u043f\u0440\u0438\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043f\u043e\u0442\u0440\u0430\u0442\u0438\u0442\u044c \u0442\u0440\u0438 \u0447\u0430\u0441\u0430 \u0438 \u0432\u043e\u0441\u0435\u043c\u044c \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432 Tesla \u0410100 \u043d\u0430 80 \u0413\u0411.\n\n\n\n\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0432\u0435\u0441\u0442\u0438 \u0440\u0435\u0447\u044c \u043f\u0440\u043e \u0432\u044b\u0431\u043e\u0440 GPU \u0434\u043b\u044f fine-tuning \u0438 inference.\n\n\n\n\u0412\u044b\u0431\u043e\u0440 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0434\u043b\u044f \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0430 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043c\u043d\u043e\u0433\u0438\u0445 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0441\u044f \u043f\u043e\u043c\u043e\u0433\u0443\u0442 \u043a\u0430\u043a \u0440\u0430\u0437 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043e\u0431\u043b\u0430\u0447\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0435\u0440\u0430\u0445 \u0438\u043b\u0438 \u043f\u043e\u0434\u0431\u043e\u0440 \u043a\u0430\u0440\u0442\u044b \u0434\u043b\u044f \u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0433\u043e \u0438\u043b\u0438 \u043e\u0444\u0438\u0441\u043d\u043e\u0433\u043e \u0418\u0418-\u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0430, \u0447\u0442\u043e \u0442\u0430\u043a\u0436\u0435 \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u043d\u044f\u0442\u044c \u043a\u0430\u043a\u043e\u0439 GPU \u0430\u0440\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c.\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 1. \u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u044b\u0435 GPU \u0434\u043b\u044f LLM\n### 1.1 \u041f\u0440\u0435\u043c\u0438\u0443\u043c 80 GB\n\nTesla H100 Hopper 80 GB SXM (Server PCI Express Module) \u0444\u043e\u0440\u043c\u0430\u0442. \u0421\u0430\u043c\u0430\u044f \u043c\u043e\u0449\u043d\u0430\u044f. FP16 = 204.9 TFLOPS. \u0412 \u0420\u0424 \u043e\u0442 4 \u043c\u043b\u043d \u0440\u0443\u0431\u043b\u0435\u0439.\n\n\n\n\n\n---\n\n\n\nA100 \u0441\u0430\u043c\u0430\u044f \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430\u044f \u0441 \u043e\u0431\u044a\u0435\u043c\u043e\u043c \u043f\u0430\u043c\u044f\u0442\u0438 80GB SXM \u0444\u043e\u0440\u043c\u0430\u0442. \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0432 \u0420\u043e\u0441\u0441\u0438\u0438 \u0432\u0430\u0440\u044c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043e\u0442 1.6-2 \u043c\u043b\u043d \u0440\u0443\u0431\u043b\u0435\u0439 \u0438 \u0432\u044b\u0448\u0435. FP16 = 78 TFLOPS\n### 1.2 \u041a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f GPU \u043d\u0430 80+ GB\n\u041c\u043d\u043e\u0433\u043e \u0441\u043f\u043e\u0440\u043e\u0432 \u043a\u0430\u043a\u0430\u044f \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 LLM. \u0421\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0430\u043c\u044f\u0442\u0438 \u043d\u0430 \u0431\u043e\u0440\u0442\u0443, \u0442\u0435\u043c \u043b\u0443\u0447\u0448\u0435. \u041d\u043e \u043d\u0430\u0434\u043e \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c 4\u0445 \u0432\u044b\u0448\u0435, \u0447\u0435\u043c 2\u0445 \u043f\u043e FP16. \u041f\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\u043c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0430\u0432\u0442\u043e\u0440\u043e\u0432 \u0432\u043e \u043c\u043d\u043e\u0433\u0438\u0445 deep learning \u0442\u0435\u0441\u0442\u0430\u0445 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u043b\u0438 4 \u043a\u0430\u0440\u0442\u044b \u0441 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c \u0443\u0440\u043e\u0432\u043d\u0435\u043c fp16 \u0438 \u043f\u043e 24 \u0413\u0431 \u043b\u0443\u0447\u0448\u0435 2-\u0445 RTX \u043f\u043e 48 \u0413\u0431, \u0438 \u0447\u0442\u043e 4 \u043a\u0430\u0440\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u043f\u043e \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043a\u0430\u043a 1 \u043a\u0430\u0440\u0442\u0430 \u0432 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0435 LLM.\n4 \u0445 RTX 3090 96 GB (\u0446\u0435\u043d\u0430 \u0437\u0430 1 \u0448\u0442 \u043e\u0442 200 \u0442.\u0440.). \u0421\u0430\u043c\u044b\u0439 \u044d\u043a\u043e\u043d\u043e\u043c\u043d\u044b\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u0434\u043e\u0441\u0442\u0438\u0436\u0435\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043c\u0430 96 GB. \u041e\u0431\u0449\u0438\u0439 FP16 = 140 TFLOPS. \u041f\u0438\u0442\u0430\u043d\u0438\u0435 1400 \u0412\u0442.\n\n\n\n\n\n---\n\n\n\n4 \u0445 4090 ADA 96 GB - (\u0446\u0435\u043d\u0430 \u0437\u0430 1 \u0448\u0442 \u043e\u0442 200 \u0442\u044b\u0441 \u0440\u0443\u0431., \u0438\u043b\u0438 250 \u0442\u044b\u0441 \u0440\u0443\u0431 \u043d\u0430 \u0432\u043e\u0434\u044f\u043d\u043e\u043c \u043e\u0445\u043b\u0430\u0436\u0434\u0435\u043d\u0438\u0438). \u041e\u0431\u0449\u0438\u0439 FP16 = 332 TFLOPS. \u041f\u0438\u0442\u0430\u043d\u0438\u0435 1800 \u0412\u0442.\n\n\n\n\n\n---\n\n\n\n2 \u0445 RTX 6000 ADA 96 GB 4 slot (\u0446\u0435\u043d\u0430 \u0437\u0430 1 \u0448\u0442 \u043e\u0442 500 \u0442\u044b\u0441.  \u0440\u0443\u0431\u043b\u0435\u0439). \u041e\u0431\u0449\u0438\u0439 FP16=180 TFLOPS\n\n\n\n\n\n---\n\n\n\n2 \u0445 RTX A6000 Ampere 96 GB 4 slot (\u0446\u0435\u043d\u0430 \u0437\u0430 1 \u0448\u0442 \u043e\u0442 500 \u0442\u044b\u0441. \u0440\u0443\u0431\u043b\u0435) \u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u043b\u044f \u043a\u043e\u043c\u043f\u0430\u043a\u0442\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0449\u0435\u043d\u0438\u044f \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435. \u041e\u0431\u0449\u0438\u0439 FP16 = 76 TFLOPS\n\n\n\n\n\n---\n\n\n\n4 \u0445 V100 Volta 32 GB = 128 Gb (\u0446\u0435\u043d\u0430 \u0437\u0430 1 \u0448\u0442 \u043e\u0442 1,4 \u043c\u043b\u043d \u0440\u0443\u0431 \u0432 \u0420\u0424). \u041e\u0431\u0449\u0438\u0439 FP16 = 56 TFLOPS.\n\n\n\n\n\n---\n\n\n\n### 1.3 \u041f\u0440\u043e\u0444\u0438 \u043d\u0430 40+ GB\n\n\n-\tRTX 6000 ADA 48 GB 2 slot - \u0441\u0430\u043c\u0430\u044f \u043c\u043e\u0449\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0430 2023 \u0441\u0440\u0435\u0434\u0438 48 \u0413\u0431 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u0441\u0442\u0430\u043d\u0446\u0438\u0439. FP16 = 91 TFLOPS.\n\n\n\n-\tL40 ADA 48 GB - \u043c\u043e\u0449\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0430, \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u0430 RTX 6000 ADA, \u0434\u043b\u044f \u0434\u0430\u0442\u0430\u0446\u0435\u043d\u0442\u0440\u043e\u0432. FP16=90.\n\n\n\n-\tA16 Ampere 64 GB - \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430, \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 4 GPU \u043f\u043e 16 GB \u0438 \u0432\u0441\u0435 \u044d\u0442\u043e \u0432 2 slot. \u041e\u0431\u0449\u0438\u0439 \u0441\u043b\u0430\u0431\u044b\u0439 FP16 = 17.9. \u0412 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0442\u0441\u044f \u043a\u0430\u043a 4 GPU. 250W\n\n\n\n-\tRTX A6000 48 GB Ampere 2 slot.\n\n\n\n-\tQUADRO RTX 8000 48 \u0413\u0431 Turing 2 Slot 2 \u0445 4090 ADA 7 Slot\n\n\n\n-\t2 \u0445 RTX 3090 Ampere 5 Slot = 48 \u0413\u0431.\n\n### 1.4 \u041b\u044e\u0431\u0438\u0442\u0435\u043b\u044c-\u042d\u043d\u0442\u0443\u0437\u0438\u0430\u0441\u0442 24+ GB\n\n\n-\t4090 ADA 24 GB 3.5 Slot - FP16 = 83 TFLOPS.\n\n\n\n-\tRTX 3090 24 GB Ampere 2 Slot FP16 = 40 TFLOPS.\n\n\n\n-\tRTX 3090 TI 24 GB Ampere 3-3.5 slot\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 2. \u041a\u043b\u0430\u0441\u0441\u044b \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 Nvidia\n\u041e\u0442 \u043a\u043b\u0430\u0441\u0441\u0430 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u0438\u0445 \u0444\u043e\u0440\u043c-\u0444\u0430\u043a\u0442\u043e\u0440, \u044d\u043d\u0435\u0440\u0433\u043e\u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c, \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043a\u043e\u0440\u0440\u0435\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043e\u043a \u043f\u0430\u043c\u044f\u0442\u0438 ECC, \u0438\u043d\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0441 \u043f\u043b\u0430\u0432\u0430\u044e\u0449\u0435\u0439 \u0442\u043e\u0447\u043a\u043e\u0439, \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c, [\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0432\u0438\u0440\u0442\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438](https://docs.nvidia.com/grid/gpus-supported-by-vgpu.html) \u0438 \u0442.\u0434.\n\n\n\n\n\n-\t**\u041a\u0430\u0440\u0442\u044b \u0434\u043b\u044f \u0434\u0430\u0442\u0430\u0446\u0435\u043d\u0442\u0440\u043e\u0432** \u2014 A100, H100, A10, A40, L40 \u0438 \u0434\u0440. \u041f\u0440\u0438\u0437\u0432\u0430\u043d\u044b \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c 24/7. \u0427\u0430\u0441\u0442\u043e \u0441\u0430\u043c\u044b\u0435 \u0434\u043e\u0440\u043e\u0433\u0438\u0435 \u043f\u043e \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438. \u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043c\u0435\u044e\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u043f\u043e \u043e\u0431\u044a\u0435\u043c\u0443 \u043f\u0430\u043c\u044f\u0442\u0438 (\u0431\u043e\u043b\u0435\u0435 80 \u0413\u0431), \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u044d\u043d\u0435\u0440\u0433\u043e\u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438. \u041c\u043e\u0436\u043d\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0441\u0443\u043f\u0435\u0440\u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u044b \u0438\u0437 \u0442\u044b\u0441\u044f\u0447 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442. \u0417\u0430\u0447\u0430\u0441\u0442\u0443\u044e \u0438\u0445 \u043b\u0443\u0447\u0448\u0435 \u0430\u0440\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c, \u0447\u0435\u043c \u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0432 \u0441\u0432\u043e\u0439 \u0441\u0435\u0440\u0432\u0435\u0440. \u0427\u0430\u0441\u0442\u043e \u043d\u0435\u0442 \u0432\u0438\u0434\u0435\u043e\u0432\u044b\u0445\u043e\u0434\u0430. \u0420\u0435\u0434\u043a\u043e \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 \u0431/\u0443, \u0432 \u0446\u0435\u043d\u0435 \u043f\u0430\u0434\u0430\u044e\u0442 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u043e.\n\n\n\n\n\n-\t**\u0414\u043b\u044f \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u0441\u0442\u0430\u043d\u0446\u0438\u0439** \u2014 RTX A6000, RTX A5500, \u0441\u0435\u0440\u0438\u044f Quadro \u0438 \u0434\u0440. \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0435\u0448\u0435\u0432\u043b\u0435, \u0447\u0435\u043c \u0434\u043b\u044f \u0434\u0430\u0442\u0430\u0446\u0435\u043d\u0442\u0440\u043e\u0432. \u0412\u0438\u0434\u043d\u0430 \u0437\u0430\u0431\u043e\u0442\u0430 \u043f\u0440\u043e \u0444\u043e\u0440\u043c-\u0444\u0430\u043a\u0442\u043e\u0440 \u0438 \u044d\u043d\u0435\u0440\u0433\u043e\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435, \u0443\u0440\u0435\u0437\u0430\u043d\u043d\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0432\u0438\u0440\u0442\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 vGPU. \u0427\u0430\u0441\u0442\u043e \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 \u0431/\u0443, \u043f\u0430\u0434\u0430\u044e\u0442 \u0432 \u0446\u0435\u043d\u0435 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u043e.\n\n\n\n-\t**\u0414\u043b\u044f \u0433\u0435\u0439\u043c\u0435\u0440\u043e\u0432** \u2014 GeForce RTX 4090, RTX 3090 \u0438 \u043f\u0440\u043e\u0447\u0438\u0435. \u041e\u0434\u043d\u0438 \u0438\u0437 \u0441\u0430\u043c\u044b\u0445 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0445 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0434\u043b\u044f \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0434\u0435\u0448\u0435\u0432\u043b\u0435 \u0432 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437. \u0421\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u043f\u043e\u0444\u0438\u0433 \u043d\u0430 \u0444\u043e\u0440\u043c-\u0444\u0430\u043a\u0442\u043e\u0440\u044b \u0438 \u044d\u043d\u0435\u0440\u0433\u043e\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435. \u0412\u043e \u043c\u043d\u043e\u0433\u0438\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445, \u043f\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0433\u0443\u0442 \u043d\u0435 \u0443\u0441\u0442\u0443\u043f\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u043c \u043a\u043b\u0430\u0441\u0441\u0430\u043c \u0438\u043b\u0438 \u043f\u0440\u0435\u0432\u043e\u0441\u0445\u043e\u0434\u0438\u0442\u044c. \u0420\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u044b \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 \u0431/\u0443, \u043f\u0430\u0434\u0430\u044e\u0442 \u0432 \u0446\u0435\u043d\u0435 \u0431\u044b\u0441\u0442\u0440\u0435\u0435, \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0441 \u0432\u044b\u0445\u043e\u0434\u043e\u043c \u043d\u043e\u0432\u044b\u0445 \u0444\u043b\u0430\u0433\u043c\u0430\u043d\u043e\u0432. \u0415\u0441\u0442\u044c \u0440\u0438\u0441\u043a \u043d\u0430\u0442\u043a\u043d\u0443\u0442\u044c\u0441\u044f \u043d\u0430 \u043a\u0430\u0440\u0442\u044b \u0441\u0442\u043e\u044f\u0449\u0438\u0435 \u0432 \u043c\u0430\u0439\u043d\u0438\u043d\u0433\u0435.\n\n\n\n\u041d\u0443\u0436\u043d\u043e \u0438\u043c\u0435\u0442\u044c \u0432\u0432\u0438\u0434\u0443, \u0447\u0442\u043e Nvidia \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 GPU \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u0438\u0440\u0443\u0435\u0442 \u0434\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 \u0432 \u043f\u0440\u043e\u0434\u0430\u043a\u0448\u0435\u043d\u0435. \u0422\u0430\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0447\u0430\u0449\u0435 \u0441\u043b\u0430\u0431\u0435\u0435 \u0431\u0430\u0437\u0438\u0441\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 \u0442\u0440\u0435\u0439\u043d\u0435 \u0438\u043b\u0438 \u0432 \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f\u0445. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, NVIDIA T4 \u0438\u043c\u0435\u0435\u0442 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 FP16 = 65 TFLOPS, \u043d\u043e \u043e\u0447\u0435\u043d\u044c \u043d\u0438\u0437\u043a\u0438\u0439 FP32 = 8 TFLOPS.\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 3. \u041c\u0438\u043a\u0440\u043e\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b \u0438 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u0441\u0442\u044c\nNvidia \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u043d\u0430 \u0440\u044b\u043d\u043e\u043a \u0431\u043e\u043b\u0435\u0435 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u0435 \u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043c\u0438\u043a\u0440\u043e\u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442. \u041e\u0442\u043b\u0438\u0447\u0438\u0439 \u043c\u043d\u043e\u0433\u043e. \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0447\u0438\u0441\u043b\u043e \u0442\u0440\u0430\u043d\u0437\u0438\u0441\u0442\u043e\u0440\u043e\u0432, \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043d\u043e\u0432\u044b\u0435 \u043f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u044f \u044f\u0434\u0435\u0440 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e CUDA / RT / Tensor \u044f\u0434\u0435\u0440, \u043f\u043e\u0432\u044b\u0448\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043d\u0430\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0438 \u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438, \u0440\u0430\u0437\u043c\u0435\u0440 L1 Cache, \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u0436\u0438\u043c\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0442\u0438\u043f\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 bloat16, INT8, mixed-precision.\n\n\n\n\n\n\u0423\u0441\u0442\u0430\u0440\u0435\u0432\u0448\u0438\u0435 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u043c\u043e\u0433\u0443\u0442 \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u0442\u044c\u0441\u044f \u0441 \u0440\u0430\u0437\u043d\u044b\u043c\u0438 \u043d\u0435\u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u043c\u043e\u0441\u0442\u044f\u043c\u0438 \u0441 CUDA \u0434\u0440\u0430\u0439\u0432\u0435\u0440\u0430\u043c\u0438 \u0438 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430\u043c\u0438 HF. \u041a\u043e\u0434 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0439 \u0434\u043b\u044f \u0441\u0432\u0435\u0436\u0435\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u0441\u044f \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0434\u0440\u0435\u0432\u043d\u0438\u0445.\n\n\n\n\n\n\u0423\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430 \u0441 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c \u043d\u043e\u043c\u0435\u0440\u043e\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0440\u0435\u0432\u0438\u0437\u0438\u0438 \u0438\u043c\u0435\u044e\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 \u044f\u0434\u0440\u0430. \u0421\u043f\u0438\u0441\u043e\u043a \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440 \u0437\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 12 \u043b\u0435\u0442:\n\n\n\n-\tHopper (2022) - Compute 9.x\n\n-\tAda Lovelace (2022) - Compute 9.x\n\n-\tAmpere (2020) - Compute 8.x\n\n-\tTuring (2018) - Compute 7.x\n\n-\tPascal (2016) - Compute 6\n\n-\tMaxwell (2014) - Compute 5\n\n-\tKepler (2012) - Compute 3\n\n\n\n\n\n\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b Ampere \u0438 Ada Lovelace.\n### 3.1 \u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 Compute Capability\n\u041a\u0430\u0436\u0434\u0430\u044f \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430 \u0438\u043c\u0435\u0435\u0442 \u0441\u0432\u043e\u0439 \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0439 Compute Capability (\u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438).\n\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u0438 https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n### 3.2 \u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439\n\u042d\u0432\u043e\u043b\u044e\u0446\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043a\u043e\u043c\u0430\u043d\u0434 \u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0435\u043c\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0435\u043d\u0437\u043e\u0440\u043d\u044b\u0445 \u044f\u0434\u0435\u0440 \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u043e\u043a\u043e\u043b\u0435\u043d\u0438\u044f\u0445 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430. https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html\n\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f Compute Capability \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430 \u0434\u043b\u044f \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b \u043c\u043e\u0436\u043d\u043e \u0437\u0434\u0435\u0441\u044c: https://developer.nvidia.com/cuda-gpus\n## \u0420\u0430\u0437\u0434\u0435\u043b 4. \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c GPU\n\u0412\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b \u0438\u043c\u0435\u044e\u0442 \u0440\u0430\u0437\u043d\u0443\u044e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u0445 \u0441 \u043f\u043b\u0430\u0432\u0430\u044e\u0449\u0435\u0439 \u0442\u043e\u0447\u043a\u043e\u0439 - FP64, FP32, FP16. \u0422\u0430\u043a\u0436\u0435 \u0435\u0441\u0442\u044c \u0435\u0449\u0435 \u043e\u0431\u0449\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0442\u0435\u043d\u0437\u043e\u0440\u043d\u044b\u0445 \u0438 RT \u044f\u0434\u0435\u0440.\n\n\n\n\u041e\u0431\u0440\u0430\u0449\u0430\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0443\u0436\u043d\u043e \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u0432 FP16 non-Tensor (half-precision). \u0411\u043e\u043b\u044c\u0448\u0435 \u044d\u0442\u043e\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0438\u043c\u0435\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441 LLM. \u041e\u0442 \u043d\u0435\u0433\u043e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u043d\u0435\u043a\u0432\u0430\u043d\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445. \u042f \u0438\u0437\u0443\u0447\u0438\u043b \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0434\u0435\u0441\u044f\u0442\u043a\u0438 \u0440\u0430\u0437\u043d\u044b\u0445 \u043a\u0430\u0440\u0442 \u0438 \u0438\u043c\u0435\u043d\u043d\u043e \u043f\u043e FP16 \u043b\u0435\u0433\u043a\u043e \u0431\u044b\u043b\u043e \u0441\u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435.\n\n\n\n```FLOPS``` (floating point operations per second) \u2014 \u0432\u043d\u0435\u0441\u0438\u0441\u0442\u0435\u043c\u043d\u0430\u044f \u0435\u0434\u0438\u043d\u0438\u0446\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043e\u0432, \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0430\u044f, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0441 \u043f\u043b\u0430\u0432\u0430\u044e\u0449\u0435\u0439 \u0437\u0430\u043f\u044f\u0442\u043e\u0439 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u0430\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0430. \u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e 1 TFLOPS \u044d\u0442\u043e 1 \u0442\u0440\u0438\u043b\u043b\u0438\u043e\u043d \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443.\n\n\n\n\n\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c FP16 \u044d\u0442\u043e \u0431\u043e\u043b\u0435\u0435 20 TFLOPS.\n\n\n\n\u0422\u043e\u043f\u043e\u0432\u044b\u0435 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b:\n\n-\tH100 Hopper 80 GB. FP16 = 204.9 TFLOPS\n\n-\tRTX 6000 Ada - FP16 = 91 TFLOPS\n\n-\t4090 ADA 24 GB 3.5 Slot FP16 = 83\n\n\n\n\n\n\u041e\u0434\u043d\u0430\u043a\u043e \u043d\u0435\u043b\u044c\u0437\u044f \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c \u043f\u0440\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u043f\u0440\u0438\u0440\u043e\u0441\u0442, \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430 \u0441\u0447\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f TFLOPS. \u041d\u043e \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 TFLOPS \u0442\u0435\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u0443\u0435\u043c\u043e \u043c\u043e\u0449\u043d\u0435\u0435 \u043a\u0430\u0440\u0442\u0430.\n\n\n\n\n\n\u041c\u0435\u0442\u043e\u0434\u044b \u0437\u0430\u043f\u0443\u0441\u043a\u0430 LLM \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f \u043f\u043e \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438. \u0417\u0430\u043f\u0443\u0441\u043a \u0441 \u043a\u0432\u0430\u043d\u0442\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u0432 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438 \u0442\u0430\u043c \u0432\u0441\u0442\u0443\u043f\u0430\u044e\u0442 \u0432 \u0441\u0438\u043b\u0443 \u0434\u0440\u0443\u0433\u0438\u0435 \u0444\u0430\u043a\u0442\u043e\u0440\u044b, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c CPU \u0438 \u043f\u0430\u043c\u044f\u0442\u0438.\n\n**\u0411\u0435\u043d\u0447\u043c\u0430\u0440\u043a llama 65B \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439**\n\u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445/\u0441\u0435\u043a\u0443\u043d\u0434\u0443 \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 200 \u0438\u043b\u0438 1900 \u043d\u043e\u0432\u044b\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432: https://www.reddit.com/r/LocalLLaMA/comments/14s7j9j/llama_65b_gpu_benchmarks/\n### 4.1 \u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432 Deep learning (\u043d\u0435 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e \u0432 LLM)\n\u041f\u043e \u0434\u0430\u043d\u043d\u044b\u043c lambdalabs.com (https://lambdalabs.com/gpu-benchmarks)\n\u0418\u0437 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0434\u043e\u0440\u043e\u0433\u0438\u0435 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b \u0442\u0438\u043f\u0430 A100 \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 10-\u0442\u0438 \u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u0438 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u0432 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u043a\u0430\u043a \u043c\u043d\u043e\u0433\u043e\u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u0438\u0445 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 (\u043f\u043e \u0442\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044f\u043c \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043e\u043d\u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0430 \u044d\u0442\u043e \u0440\u0430\u0437\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438).\n### 4.2 \u0411\u0443\u0434\u0435\u0442 \u043b\u0438 \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u0438\u043b\u0438 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441 \u043f\u0440\u0438 \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 \u043a\u0430\u0440\u0442\u0430\u043c\u0438?\n\u041f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432 \u0432 \u0445\u043e\u0434\u0435 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438, \u0447\u0442\u043e \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441 \u043d\u0435 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043e\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442, \u043d\u043e \u0438 \u043d\u0435 \u0434\u0435\u0433\u0440\u0430\u0434\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0438 device_map=auto. \u041a\u0430\u043a \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u043c\u043e\u0436\u043d\u043e \u043e\u0436\u0438\u0434\u0430\u0442\u044c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 \u043a\u0430\u043a \u0443 1 \u043a\u0430\u0440\u0442\u044b, \u043d\u043e \u0441 \u0431\u043e\u043b\u044c\u0448\u0435\u0439 \u043f\u0430\u043c\u044f\u0442\u044c\u044e.\n**7B - fp16**\n\n- RTX 8000 - 15 c.\n\n- 2\u0445RTX 8000 - 15 c.\n\n\n\n**13B - fp16**\n\n- 1\u0445RTX 8000 - 26 c.\n\n- 2\u0445RTX 8000 - 27 c.\n\n- 2\u0445RTX 8000 + RTX 3090 - 27 c.\n\n- 1\u0445RTX 8000 + RTX 3090 - 24 c.\n\n\n\n**13B - fp32** (\u043d\u0435 \u0432\u043b\u0435\u0437\u0435\u0442 \u043d\u0430 1 \u043a\u0430\u0440\u0442\u0443, \u0438\u0442\u043e\u0433\u043e \u043f\u043e 30\u0413\u0431 \u043d\u0430 \u043a\u0430\u0440\u0442\u0443)\n\n- 2\u0445RTX 8000 - 50 c.\n\n\n\n**70B - 4bit** - \u043f\u043e 22\u0413\u0431 \u043d\u0430 \u043a\u0430\u0440\u0442\u0443.\n\n- 1\u0445RTX 8000 - 48 c.\n\n- 2\u0445RTX 8000 - 48 c.\n\n### 4.3 \u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0442\u0432\u0435\u0442\u0430 \u043f\u043e \u043d\u0430\u0448\u0438\u043c \u0442\u0435\u0441\u0442\u0430\u043c \u043d\u0430 \u042f\u043d\u0434\u0435\u043a\u0441 Cloud \u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0435 A100 80GB.\n**\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438**\n\n\n\n\n\n\n\n-\tMultiple NVIDIA GPUs or Apple Silicon for Large Language Model Inference? (https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference)\n\n\n\n-\t\u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c Whisper (https://github.com/openai/whisper/discussions/918)\n\n\n\n-\tGeForce RTX 4090 vs RTX 3090 Ti vs 2080 Ti (https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf)\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 5. \u041e\u0431\u044a\u0435\u043c \u0432\u0438\u0434\u0435\u043e\u043f\u0430\u043c\u044f\u0442\u0438 GPU\n\u042d\u0432\u043e\u043b\u044e\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u043b\u0435\u0442 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442: \u0441 \u043e\u0434\u043d\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u0441\u0435 \u0431\u043e\u043b\u0435\u0435 \u0442\u044f\u0436\u0435\u043b\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0441\u043e\u0442\u043d\u0438 \u043c\u0438\u043b\u043b\u0438\u0430\u0440\u0434\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0441 \u0434\u0440\u0443\u0433\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u0441\u0435 \u0431\u043e\u043b\u0435\u0435 \u043b\u0435\u0433\u043a\u0438\u0435 \u0438 \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u043c\u044b\u0435 \u043f\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0443 \u0441 \u0431\u043e\u043b\u0435\u0435 \u0442\u044f\u0436\u0435\u043b\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438. Mistral-7B \u043f\u043e\u0431\u0435\u0436\u0434\u0430\u0435\u0442 llama2-13B \u043f\u043e \u043c\u043d\u043e\u0433\u0438\u043c \u0442\u0435\u0441\u0442\u0430\u043c.\n\n\n\n**\u0413\u043b\u0430\u0432\u043d\u044b\u0439 \u0442\u0435\u0437\u0438\u0441** - \u043f\u0430\u043c\u044f\u0442\u0438 \u043c\u043d\u043e\u0433\u043e \u043d\u0435 \u0431\u044b\u0432\u0430\u0435\u0442, \u043d\u0438\u0436\u0435 24 \u0413\u0431 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f, \u0430 48 \u0413\u0431 \u0441\u0438\u043b\u044c\u043d\u043e \u0432\u044b\u0440\u0443\u0447\u0430\u0435\u0442 \u043f\u0440\u0438 \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u044f\u0445.\n\n\n\n**\u0422\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u044f** \u0432 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u0438 \u043f\u0430\u043c\u044f\u0442\u0438 \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0441\u0442\u0438. \u0412 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u043b\u043e\u0433\u0438\u0447\u043d\u043e, \u0447\u0442\u043e \u0441\u0430\u043c\u043e\u0435 \u043f\u0435\u0440\u0435\u0434\u043e\u0432\u043e\u0435 \u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u0442\u044f\u0436\u0435\u043b\u044b\u043c \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u0432\u0440\u0435\u043c\u044f.\n\n\n\n**\u0427\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043f\u0430\u043c\u044f\u0442\u044c?**\n\n- \u0427\u0438\u0441\u043b\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432 \u043c\u043e\u0434\u0435\u043b\u0438 - 7B, 13B, 40B, 70B \u0438 \u0442.\u0434.\n\n- \u0424\u043e\u0440\u043c\u0430\u0442\u044b float16, bfloat16, \u043a\u0432\u0430\u043d\u0442\u0438\u0437\u0430\u0446\u0438\u0438 8bit, 4bit\n\n- \u0414\u043b\u0438\u043d\u0430 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u043f\u0440\u043e\u043c\u043f\u0442\u0435. \u041c\u043e\u0434\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u0441\u044f \u0432 \u043f\u0430\u043c\u044f\u0442\u044c, \u043d\u043e \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u043e\u0434\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u043d\u0430 \u0432\u0445\u043e\u0434\u0435, \u0442\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043f\u0430\u043c\u044f\u0442\u0438 \u0434\u043b\u044f \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438. \u041c\u043e\u0434\u0435\u043b\u044c 13B \u0432 fp16 \u0435\u0441\u043b\u0438 \u0432 \u043d\u0435\u0451 \u043a\u0438\u0434\u0430\u0442\u044c \u0434\u043b\u0438\u043d\u043d\u044b\u0439 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 8k \u043b\u0435\u0433\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 80 \u0413\u0411.\n\n- \u0410\u0434\u0430\u043f\u0442\u0435\u0440\u044b \u043a pre-train \u043c\u043e\u0434\u0435\u043b\u044f\u043c \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0430\u043c\u044f\u0442\u0438\n\n- \u041f\u0440\u0438 fine tuning \u043f\u0430\u043c\u044f\u0442\u044c \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b QLoRA\n\n**\u041c\u043e\u0436\u043d\u043e \u043b\u0438 c 16 \u0413\u0431 Vram \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c?**\n\n\n\n\u0414\u0430, \u043c\u043e\u0436\u043d\u043e \u0434\u043e 13B \u0441 4\u0431\u0438\u0442 \u043a\u0432\u0430\u043d\u0442\u0438\u0437\u0430\u0446\u0438\u0435\u0439, \u0438 \u043d\u0435\u0434\u043e\u043b\u0433\u043e - \u0447\u0430\u0441\u0442\u043e \u0437\u0430\u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435\u0433\u043e. \u0421\u043c \u0432\u043d\u0438\u0437\u0443 \u0440\u0430\u0437\u0434\u0435\u043b \u0442\u0435\u0441\u0442\u043e\u0432 \u043d\u0430 16 \u0413\u0431 - [What other consequences are there?](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n\n**\u0421\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0430\u043c\u044f\u0442\u0438 \u043d\u0443\u0436\u043d\u043e \u0434\u043b\u044f fine-tuning?**\n\n\n\n7B \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0438 \u0442\u044e\u043d\u0438\u043d\u0433\u0435 \u0432 4 \u0431\u0438\u0442\u0430\u0445 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 16 \u0413\u0431 \u043f\u0440\u0438 \u0442\u0440\u0435\u0439\u043d\u0435, \u0438 20 \u0413\u0431 \u043f\u0440\u0438 \u0441\u043b\u0438\u044f\u043d\u0438\u0438, \u043d\u043e \u0435\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0435 \u0434\u0435\u043b\u0430\u0442\u044c, \u043e\u0441\u0442\u0430\u0432\u0438\u0432 \u043b\u0438\u0448\u044c \u0430\u0434\u0430\u043f\u0442\u0435\u0440. lora_r=16, lora_alpha=32\n\n**\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0430\u043c\u044f\u0442\u0438 Llama 13B 8K**\n\n\n\n\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 [HF](https://huggingface.co/docs/transformers/main_classes/pipelines) (Huggingface) [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) \u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435. \u0412 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 2 GPU 3090 24 GB. \u0418\u0437 \u0442\u0435\u0441\u0442\u0430 \u0432\u0438\u0434\u043d\u043e: \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e 16 \u041c\u0431 \u043f\u0430\u043c\u044f\u0442\u0438 \u043f\u0440\u0438 float16 \u043d\u0430 \u0434\u0432\u0435 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b. 5 \u041c\u0431 / \u0441\u043b\u043e\u0432\u043e \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0435 \u0441 8 \u0431\u0438\u0442. 22 \u041c\u0431 / \u0441\u043b\u043e\u0432\u043e \u043d\u0430 \u0434\u0432\u0443\u0445 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430\u0445 8 \u0431\u0438\u0442 \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0440\u0443\u0441\u0441\u043a\u0438\u0445 \u0441\u043b\u043e\u0432 \u043a \u0442\u043e\u043a\u0435\u043d\u0430\u043c 2,8. \u041c\u043e\u0434\u0435\u043b\u044c \u0438\u043c\u0435\u0435\u0442 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 8K \u0442\u043e\u043a\u0435\u043d\u043e\u0432.\n\n\u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\n**\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438**\n\n\n\n- \u0421\u043f\u0440\u0430\u0432\u043e\u0447\u043d\u0438\u043a [\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f GPU \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b LLM](https://huggingface.co/spaces/Vokturz/can-it-run-llm)\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 6. \u041f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n\u041f\u0440\u0435\u0436\u0434\u0435 \u0447\u0435\u043c \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0439 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440 \u0438 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0443 \u043b\u0443\u0447\u0448\u0435 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0434\u0443\u043c\u0430\u0442\u044c \u043e \u043f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u0430\u0445 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u043a\u043e\u043c\u043f\u0440\u043e\u043c\u0438\u0441\u0441 \u043c\u0435\u0436\u0434\u0443 \u0446\u0435\u043d\u043e\u0439, \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u043a\u0430\u0440\u0442, \u043e\u0431\u044a\u0435\u043c\u043e\u043c \u043f\u0430\u043c\u044f\u0442\u0438 \u0438 \u044d\u043d\u0435\u0440\u0433\u043e\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435\u043c. \u041c\u043e\u0436\u043d\u043e \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u043c\u044b\u0441\u043b\u0438, \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0441\u043c\u0435\u043d\u0438\u0442\u044c \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0443, \u0447\u0435\u043c \u043f\u044b\u0442\u0430\u0442\u044c\u0441\u044f \u0443\u043c\u0435\u0441\u0442\u0438\u0442\u044c \u0432\u0441\u0435 \u0432 \u0441\u0442\u0430\u0440\u044b\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u043d\u0438\u043a.\n\n\n\n\u041e\u0434\u043d\u0430 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430 3090/4090 \u043d\u0430 24 \u0413\u0431 \u0432 \u043f\u043e\u043b\u0435 \u0432\u043e\u0438\u043d. \u0414\u0432\u0435 \u043a\u0430\u0440\u0442\u044b \u043f\u043e 24 \u0413\u0431 \u043e\u0447\u0435\u043d\u044c \u0432\u044b\u0440\u0443\u0447\u0430\u044e\u0442, \u0430 \u0432\u043e\u0442 \u0434\u0432\u0435 \u043f\u043e 48 \u0413\u0431 \u0431\u044b\u043b\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u043a\u043b\u0430\u0441\u0441. \u041f\u0440\u043e \u043e\u0434\u043d\u0443 \u0441 80 \u0413\u0431 \u0443\u043c\u043e\u043b\u0447\u0438\u043c \u043f\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u043c \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u043c. \u041d\u043e \u043a\u043e\u0433\u0434\u0430 \u0431\u044e\u0434\u0436\u0435\u0442 \u043d\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442, \u0430 \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u043c\u043d\u043e\u0433\u043e \u043f\u0430\u043c\u044f\u0442\u0438 - \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u0445\u043e\u0442\u0435\u0442\u044c 3-4 GPU \u043f\u043e 24 \u0433\u0431.\n\n\n\n**\u0414\u0440\u0430\u0439\u0432\u0435\u0440\u0430**. \u0423 \u043d\u0430\u0441 \u043f\u043e\u043a\u0430 \u043d\u0435\u0442 \u0441\u0432\u0435\u0434\u0435\u043d\u0438\u0439 \u0435\u0441\u043b\u0438 \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 \u0441\u0442\u043e\u044f\u0442 \u043a\u0430\u0440\u0442\u044b \u0440\u0430\u0437\u043d\u044b\u0445 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, Pascal \u0438 Amper, \u0431\u0443\u0434\u0443\u0442 \u043b\u0438 \u043a\u043e\u043d\u0444\u043b\u0438\u043a\u0442\u044b \u0434\u0440\u0430\u0439\u0432\u0435\u0440\u043e\u0432 \u0438 \u043d\u0435 \u043f\u0440\u0438\u0434\u0435\u0442\u0441\u044f \u043b\u0438 \u0441\u0438\u0434\u0435\u0442\u044c \u043d\u0430 \u0443\u0441\u0442\u0430\u0440\u0435\u0432\u0448\u0438\u0445 \u0432\u0435\u0440\u0441\u0438\u044f\u0445 CUDA \u0432\u0441\u0435 \u043d\u043e\u0432\u044b\u043c \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430\u043c. RTX 8000 Turning \u0438 3090 Ampere \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u043d\u0430 \u043e\u0434\u043d\u0438\u0445 \u0434\u0440\u0430\u0439\u0432\u0435\u0440\u0430\u0445.\n\n### 6.1 PCIE \u0438 \u043a\u043e\u0440\u043f\u0443\u0441\n**PCI-E \u0441\u043b\u043e\u0442\u044b**. \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043c\u0430\u0442\u0435\u0440\u0438\u043d\u0441\u043a\u0438\u0435 \u043f\u043b\u0430\u0442\u044b \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u0430\u044e\u0442 2-5 PCIE x16 \u0441\u043b\u043e\u0442\u043e\u0432. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u043f\u043b\u0430\u0442\u0435 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c 1 \u0441\u043b\u043e\u0442 Pcie-5, 2 \u0441\u043b\u043e\u0442 Pcie-4, 2 \u0441\u043b\u043e\u0442\u0430 Pcie-3. \u041c\u0430\u0442\u0435\u0440\u0438\u043d\u0441\u043a\u0438\u0435 \u043f\u043b\u0430\u0442\u044b \u0434\u043b\u044f \u0441\u0435\u0440\u0432\u0435\u0440\u043e\u0432 \u0438 \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u0441\u0442\u0430\u043d\u0446\u0438\u0439 \u043c\u043e\u0433\u0443\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0441\u0440\u0430\u0437\u0443 4-7 PCIE 5 x16 - \u0442\u0430\u043a\u043e\u0435 \u043f\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u0442\u043e\u0438\u0442 \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u044c.\n\n- \t\u0415\u0441\u043b\u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u043c\u044b\u0439 \u043f\u0440\u0435\u0434\u0435\u043b \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 2 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b - \u043f\u043e\u0439\u0434\u0435\u0442 \u043c\u0430\u0442\u0435\u0440\u0438\u043d\u0441\u043a\u0430\u044f \u043f\u043b\u0430\u0442\u0430 \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u043e 2 \u0441\u043b\u043e\u0442\u0430 4 \u0438\u043b\u0438 5\u0439 \u0432\u0435\u0440\u0441\u0438\u0438 \u043f\u043e \u044516.\n\n- \t\u0415\u0441\u043b\u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0441\u044f 3-4 \u043a\u0430\u0440\u0442\u044b \u043d\u0435 \u0441\u043e\u0432\u0435\u0442\u0443\u0435\u043c \u043f\u043b\u0430\u0442\u044b \u0433\u0434\u0435 \u043f\u043e\u0441\u043b\u0435 2\u0445 \u0441\u043b\u043e\u0442\u043e\u0432 4 \u0438\u043b\u0438 5\u0439 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 - 3 \u0432\u0435\u0440\u0441\u0438\u0438 PCIE. \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0432\u043f\u043b\u043e\u0442\u044c \u0434\u043e \u043f\u043e\u0442\u0435\u0440\u0438 \u0432\u044b\u0432\u043e\u0434\u0430 \u0441 \u043c\u043e\u0434\u0435\u043b\u0438.\n\n- \t\u0418\u0434\u0435\u0430\u043b\u044c\u043d\u043e \u043a\u043e\u0433\u0434\u0430 \u0432\u0441\u0435 \u0441\u043b\u043e\u0442\u044b x16 \u043e\u0434\u043d\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0438 \u0438 \u0440\u0430\u0441\u043f\u0430\u044f\u043d\u044b \u043f\u043e 16 \u043b\u0438\u043d\u0438\u0439. \u041d\u043e \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u044d\u0442\u043e \u0441\u0435\u0440\u0432\u0435\u0440\u043d\u044b\u0435 \u043c\u0430\u0442\u0435\u0440\u0438\u043d\u043a\u0438 \u043d\u0435 \u0434\u043b\u044f \u043f\u043e\u0442\u0440\u0435\u0431\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432.\n\n\n\n**PCI-E \u043b\u0438\u043d\u0438\u0438 \u043d\u0430 \u043c\u0430\u0442\u0435\u0440\u0438\u043d\u0441\u043a\u0438\u0445 \u043f\u043b\u0430\u0442\u0430\u0445.** \u0415\u0441\u043b\u0438 \u043d\u0430 \u043f\u043b\u0430\u0442\u0435 \u0435\u0441\u0442\u044c 4 \u0441\u043b\u043e\u0442\u0430 PCIE \u0441 \u0440\u0430\u0437\u044a\u0451\u043c\u043e\u043c \u043d\u0430 16 \u043b\u0438\u043d\u0438\u0439, \u044d\u0442\u043e \u043d\u0435 \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u043a \u043d\u0438\u043c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043e 16 \u043b\u0438\u043d\u0438\u0439. \u0412 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f, \u043d\u0443\u0436\u043d\u043e \u0447\u0438\u0442\u0430\u0442\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e. \u0427\u0430\u0441\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c, \u0447\u0442\u043e 1 \u0441\u043b\u043e\u0442 x16 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0432\u0441\u0435 16 \u043b\u0438\u043d\u0438\u0439, \u0430 3 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0443\u0434\u0443\u0442 \u0432 \u0440\u0435\u0436\u0438\u043c\u0435 x4.\n\n\n\n**PCI-E \u043b\u0438\u043d\u0438\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430.** \u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u0435\u0441\u0442\u044c \u0442\u0430\u043a\u0436\u0435 \u0443 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432 \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043b\u0438\u043d\u0438\u0439 PCIE, \u0442\u0435\u043c\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0430\u0442\u044c\u0438. \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430 \u0442\u0438\u043f\u0430 i9, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043c\u0435\u0435\u0442 21 PCIE \u043b\u0438\u043d\u0438\u0438 \u0445\u0432\u0430\u0442\u0438\u0442 \u0434\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 LLM \u043d\u0430 4 GPU. \u041f\u043e\u0442\u0435\u0440\u0438 \u0438\u0437-\u0437\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u0439 PCIE \u0431\u0443\u0434\u0443\u0442 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0449\u0435\u0433\u043e \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u0437 2-4 GPU.\n\n\n\n**\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438**\n\n- \t\u0422\u0435\u0441\u0442 - [PCIe X16 vs X8 with 4 x Titan V GPUs for Machine Learning](https://www.pugetsystems.com/labs/hpc/pcie-x16-vs-x8-with-4-x-titan-v-gpus-for-machine-learning-1167/)\n7x NVIDIA GeForce RTX 4090 GPU Scaling https://www.pugetsystems.com/labs/articles/1-7x-nvidia-geforce-rtx-4090-gpu-scaling/\n\n\n\n\n\n---\n\n\n\n**\u041a\u043e\u0440\u043f\u0443\u0441\u0430** \u0442\u0430\u043a\u0436\u0435 \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u043e\u0440\u043d\u044b\u0435 \u0434\u043b\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 GPU \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u043e\u0442 2.5 \u0441\u043b\u043e\u0442\u043e\u0432. \u041c\u043e\u0436\u043d\u043e \u043f\u043e\u0438\u0441\u043a\u0430\u0442\u044c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u043e\u0442 \u043f\u0440\u043e\u0444\u0438 \u0438\u043b\u0438 \u0443 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043c\u0430\u0439\u043d\u0435\u0440\u043e\u0432.\n\n\n\n\u041a\u043e\u0433\u0434\u0430 \u0434\u0432\u0435 \u0432\u0438\u0434\u0435\u043e \u043a\u0430\u0440\u0442\u044b \u0443\u0436\u0435 \u0435\u0441\u0442\u044c. \u0422\u0440\u0435\u0442\u044c\u044e \u0438 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0443\u044e \u0443\u0436\u0435 \u0441\u043b\u043e\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c, \u043a\u0440\u043e\u043c\u0435 \u043a\u0430\u043a \u0432 \u0432\u0438\u0434\u0435 \u0444\u0435\u0440\u043c\u044b \u043c\u0430\u0439\u043d\u0438\u043d\u0433\u0430. \u0412 \u043b\u044e\u0431\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0443\u0436\u0435\u043d \u0431\u0443\u0434\u0435\u0442 \u0441\u0430\u043c\u043e\u043f\u0430\u043b \u043f\u043e \u043a\u0440\u0435\u043f\u043b\u0435\u043d\u0438\u044e \u043d\u0430 \u043a\u043e\u0440\u043f\u0443\u0441, \u0435\u0441\u043b\u0438 \u0440\u0435\u0447\u044c \u043f\u043e\u0439\u0434\u0435\u0442 \u043e \u0433\u0435\u0439\u043c\u0435\u0440\u0441\u043a\u0438\u0445 \u043a\u0430\u0440\u0442\u0430\u0445 \u0438 \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u043d\u044b\u0445 \u043a\u043e\u0440\u043f\u0443\u0441\u0430\u0445 - \u043f\u043b\u0430\u0442\u0430 \u0437\u0430 \u043d\u0438\u0437\u043a\u0443\u044e \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c. \u0415\u0441\u043b\u0438 \u043d\u0443\u0436\u043d\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0442\u044c \u043c\u0435\u0441\u0442\u043e - \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 \u0434\u043e\u0440\u043e\u0433\u0438\u0435 \u043d\u0438\u0437\u043a\u043e\u043f\u0440\u043e\u0444\u0438\u043b\u044c\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u044b \u0438\u0437 \u0441\u0435\u0440\u0438\u0439 A100/A10/A40, RXT QUADRO, RTX 6000.\n\n### 6.2 \u0420\u0430\u0439\u0437\u0435\u0440\u044b PCIE \u044516\n\u0420\u0430\u0439\u0437\u0435\u0440 \u0434\u043b\u044f \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u044b Cooler Master Riser Cable PCIe 4.0 x16\n\n\n\n\n\n---\n\n\n\n\u041a\u043e\u0433\u0434\u0430 \u0435\u0441\u0442\u044c 4 PCIE \u0441\u043b\u043e\u0442\u0430 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c 4 \u043a\u0430\u0440\u0442\u044b \u043f\u043e\u043c\u043e\u0433\u0443\u0442 \u0440\u0430\u0439\u0437\u0435\u0440\u044b PCIE\u044516 gen4 gen3. \u0411\u044b\u0432\u0430\u044e\u0442 \u0434\u043b\u0438\u043d\u043e\u0439 20, 30, 40 \u0441\u043c. \u0415\u0441\u0442\u044c \u0435\u0449\u0435 60 \u0441\u043c - Lian Li 600 \u043c\u043c PCI-e 4.0 (PW-PCI-4-60X).\n### 6.3 \u0420\u0430\u0441\u0448\u0438\u0440\u0438\u0442\u0435\u043b\u0438 PCIE (PCI-Express expansion enclosure)\n\u0415\u0441\u043b\u0438 \u0432\u0434\u0440\u0443\u0433 \u043e\u0441\u0442\u0430\u043b\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e 1 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u044b\u0439 PCIE \u0441\u043b\u043e\u0442, \u0430 \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442. \u0415\u0441\u0442\u044c \u0442\u0430\u043a\u043e\u0439 \u0442\u0438\u043f \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432, \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0430\u0432\u043d\u043e \u043d\u0430 \u0440\u044b\u043d\u043a\u0435, - \u0440\u0430\u0441\u0448\u0438\u0440\u0438\u0442\u0435\u043b\u0438 PCIE. \u0412\u043e\u0442\u043a\u043d\u0443\u0432 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0432 \u0441\u043b\u043e\u0442 PCIE \u043d\u0430 \u043c\u0430\u0442\u0435\u0440\u0438\u043d\u0441\u043a\u043e\u0439 \u043f\u043b\u0430\u0442\u0435 \u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0442 4\u0445 \u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0442\u043e\u0432 PCIE.\n\n\n\n\u0412\u0441\u0435 \u043d\u0430\u0442\u0438\u0432\u043d\u043e, \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u0434\u0440\u0430\u0439\u0432\u0435\u0440\u043e\u0432 \u0438 \u0442.\u0434. \u0412 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u0433\u043e\u0434\u0443 \u0443\u0436\u0435 \u043f\u043e\u044f\u0432\u0438\u043b\u0438\u0441\u044c \u0432\u0435\u0440\u0441\u0438\u0438 PCIe 4.0 \u00d716 (256Gbps) \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 Netstor NA255A-G4 \u0438 \u0440\u044d\u043a\u043e\u0432\u044b\u0439 - NA265A-G4. \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u0434\u043e 4 \u043a\u0430\u0440\u0442 3\u0445 \u0441\u043b\u043e\u0442\u043e\u0432\u044b\u0445 \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u044c. \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u043e\u0442 500 \u0442\u044b\u0441 \u0440\u0443\u0431\u043b\u0435\u0439.\n\n### 6.4 \u041f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0441 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c \u043f\u0438\u0442\u0430\u043d\u0438\u044f\n\u041f\u0440\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0449\u043d\u044b\u0445 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442 \u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0431\u043b\u043e\u043a\u0430\u0445 \u043f\u0438\u0442\u0430\u043d\u0438\u044f, \u043b\u0438\u0431\u043e \u0432 \u043e\u0434\u043d\u043e\u043c \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0431\u043e\u043b\u0435\u0435 \u043c\u043e\u0449\u043d\u043e\u043c. \u0411\u043b\u043e\u043a \u043f\u0438\u0442\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c \u043d\u0430 40% \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u044b.\n\n\n\n\u041c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0431\u043b\u043e\u043a\u043e\u0432 \u043f\u0438\u0442\u0430\u043d\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0443\u0436\u043d\u043e \u0441\u0438\u043d\u0445\u0440\u043e\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0447\u0435\u0440\u0435\u0437 Mollex \u043a\u043e\u043d\u043d\u0435\u043a\u0442\u043e\u0440 \u0438 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0441\u0438\u043d\u0445\u0440\u043e\u043d\u0438\u0437\u0430\u0442\u043e\u0440 \u0442\u0438\u043f\u0430 ATX2ATX-N03.\n\n### 6.5 \u041f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0441 \u043e\u0445\u043b\u0430\u0436\u0434\u0435\u043d\u0438\u0435\u043c\n\u0411\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0437\u0434\u0435\u0441\u044c \u043a\u0430\u0440\u0442 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u0433\u0440\u0435\u044e\u0442\u0441\u044f - \u043f\u0440\u0438 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0430\u0445 \u0434\u043e 80-90 \u0433\u0440\u0430\u0434\u0443\u0441\u043e\u0432. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0435\u0441\u043b\u0438 1-2 \u043a\u0430\u0440\u0442\u044b \u0433\u0435\u0439\u043c\u0435\u0440\u0441\u043a\u0438\u0435 \u0435\u0449\u0435 \u043c\u043e\u0436\u043d\u043e \u0441\u0442\u0435\u0440\u043f\u0435\u0442\u044c \u0434\u043e\u043c\u0430, \u0442\u043e 4 \u044d\u0442\u043e \u0443\u0436\u0435 \u0431\u043e\u043b\u0435\u0435 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0435 \u0442\u0435\u043f\u043b\u043e\u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u0435.\n## \u0420\u0430\u0437\u0434\u0435\u043b 7. \u0421\u0442\u043e\u043f-\u043b\u0438\u0441\u0442 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442 \u0434\u043b\u044f LLM\n\u0421\u043f\u0438\u0441\u043e\u043a \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u043e\u0438\u0442 \u043e\u0431\u0445\u043e\u0434\u0438\u0442\u044c \u0441\u0442\u043e\u0440\u043e\u043d\u043e\u0439\n\n\n\n\n\n- \tNvidia Tesla P40 24GB - \u0438\u043c\u0435\u0435\u0442 \u043d\u0438\u0437\u043a\u0443\u044e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c Fp16 = 0.2 TFLOPS. \u0412 \u043e\u0434\u043d\u0443 \u043a\u0430\u0440\u0442\u0443 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c 7\u0411 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c 8bit \u043f\u0440\u0438 float32=12 TFLOPS. \u041f\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0441\u0432\u0435\u0434\u0435\u043d\u0438\u044f\u043c, \u0432 15 \u0440\u0430\u0437 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u0435\u0435 \u0447\u0435\u043c 3090 \u0432 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0435 LLM.\n\n- \tNvidia Tesla M40 24GB \u0441\u043b\u0430\u0431\u0435\u0435 P40.\n\n- \tNVIDI\u0410 \u0422\u0415SL\u0410 \u0420100 16G\u0412 \u0434\u043e\u0440\u043e\u0436\u0435 P40, Fp16 = 4.7 \u043d\u043e \u043f\u0430\u043c\u044f\u0442\u0438 \u043c\u0435\u043d\u044c\u0448\u0435.\n\n## \u0420\u0430\u0437\u0434\u0435\u043b 8. \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u0441\u0443\u0440\u0441\u044b \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u042f\u043d\u0434\u0435\u043a\u0441 Cloud\n\u041f\u0440\u0430\u0432\u0438\u043b\u0430 \u0442\u0430\u0440\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u043b\u044f Compute Cloud https://cloud.yandex.ru/ru/docs/compute/pricing\n\n\n\n\n\n---\n\n\n\n\u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0438 \u0432\u044b\u0431\u043e\u0440\u0435 \u0412\u041c \u043d\u0430 A100 80GB (\u043f\u0440\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 GPU, \u043a\u0440\u0430\u0442\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f vCPU \u0438 RAM)\n\n\n\n\n\n---\n\n\n", "metadata": {"subid": 1, "total": 2, "source": "https://colab.research.google.com/drive/1vDISDnbOB89WXz41bFhWfYGASl-QtI1g?usp=sharing"}}}]