[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n\n# \u0417\u0430\u043f\u0443\u0441\u043a \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 SoTA \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0445 (\u043a\u043e\u043d\u0442\u0443\u0440\u043d\u044b\u0445) \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438\u0437 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u043b\u0438\u0434\u0435\u0440\u043e\u0432 MTEB HuggingFace \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u0437\u0430\u0434\u0430\u0447\u0435 Sentence Similarity.\n**\u042d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438** \u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a\u0430\u043a \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u0435 (\u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0435)  \u2014 \u044d\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432\u0430. \u0412\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043f\u0440\u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0435 \u0442\u0435\u043a\u0441\u0442\u0430. \u041e\u0431\u044b\u0447\u043d\u043e \u043e\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0441 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u043d\u0430\u043a\u043e\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043a\u043e\u0434\u0438\u0440\u0443\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e \u043e\u0436\u0438\u0434\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0441\u043b\u043e\u0432\u0430, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u0435\u0441\u044f \u0431\u043b\u0438\u0436\u0435 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435, \u0431\u0443\u0434\u0443\u0442 \u0441\u0445\u043e\u0436\u0438 \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e.\n**Sentence Similarity** (\u0421\u0445\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439) \u2014 \u044d\u0442\u043e \u0437\u0430\u0434\u0430\u0447\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0445\u043e\u0436\u0438 \u0434\u0432\u0430 \u0442\u0435\u043a\u0441\u0442\u0430. \u041c\u043e\u0434\u0435\u043b\u0438 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e\u0442 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0442\u0435\u043a\u0441\u0442\u044b \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u044b (\u0432\u043b\u043e\u0436\u0435\u043d\u0438\u044f), \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0431\u0438\u0440\u0430\u044e\u0442 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u0431\u043b\u0438\u0437\u043a\u0438 (\u043f\u043e\u0445\u043e\u0436\u0438) \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439. \u042d\u0442\u0430 \u0437\u0430\u0434\u0430\u0447\u0430 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u0430 \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0438 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438/\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438.\nall-MiniLM-L6-v2\n\n\n\nall-mpnet-base-v2\n\n\n\nmultilingual-e5-large\n\n\n\nMuennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit\n\n\n\nMuennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n\n\n\nllama-2-7b.ggmlv3.q4_1\n\n\n\nllama-2-7b.ggmlv3.q8_0\n\u0422\u0435\u0441\u0442 **Massive Text Embedding Benchmark (MTEB)**.\n\n\n\nMTEB \u043e\u0445\u0432\u0430\u0442\u044b\u0432\u0430\u0435\u0442 8 \u0437\u0430\u0434\u0430\u0447 \u0432\u043d\u0435\u0434\u0440\u0435\u043d\u0438\u044f, \u043e\u0445\u0432\u0430\u0442\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0432 \u043e\u0431\u0449\u0435\u0439 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 58 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 112 \u044f\u0437\u044b\u043a\u043e\u0432. \u041f\u0443\u0442\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043d\u0430 MTEB \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0438 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043b\u043d\u044b\u0439 \u043d\u0430 \u0441\u0435\u0433\u043e\u0434\u043d\u044f\u0448\u043d\u0438\u0439 \u0434\u0435\u043d\u044c \u0442\u0435\u0441\u0442 \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430.\n\n\n\n\u0418\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0438 \u0442\u0430\u043a\u0436\u0435, \u0447\u0442\u043e \u043d\u0438 \u043e\u0434\u0438\u043d \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0434\u043e\u043c\u0438\u043d\u0438\u0440\u0443\u044e\u0449\u0438\u043c \u0432\u043e \u0432\u0441\u0435\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445. \u042d\u0442\u043e \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0432 \u044d\u0442\u043e\u0439 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u0435\u0449\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u0438\u0439\u0442\u0438 \u043a \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u043c\u0435\u0442\u043e\u0434\u0443 \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0432 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e\u0439 \u0441\u0442\u0435\u043f\u0435\u043d\u0438, \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0441\u0430\u043c\u044b\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0437\u0430\u0434\u0430\u0447 \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u044f.\n\n\n\nMTEB \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u043c \u043a\u043e\u0434\u043e\u043c \u0438 \u043e\u0431\u0449\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0439 \u0442\u0430\u0431\u043b\u0438\u0446\u0435\u0439 \u043b\u0438\u0434\u0435\u0440\u043e\u0432 \u043f\u043e \u044d\u0442\u043e\u043c\u0443 URL-\u0430\u0434\u0440\u0435\u0441\u0443 https://huggingface.co/spaces/mteb/leaderboard\n# Sentence Transformers, Llama-cpp-python, OpenAI\n## \u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\n1. **Sentence Transformers** **(Hugging Face)** \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u0430 \u0438 \u043e\u0447\u0435\u043d\u044c \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u0430 \u0434\u043b\u044f \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439, \u0430\u0431\u0437\u0430\u0446\u0435\u0432 \u0438 \u0446\u0435\u043b\u044b\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432. \u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043d\u0430\u0439\u0442\u0438 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0442\u043d\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 Sentence Transformers \u0438\u0437 HuggingFace Hub. \u041c\u0435\u0442\u043e\u0434 **HuggingFaceEmbeddings, SentenceTransformerEmbeddings** \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e **LangChain**. https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers\n\n\n\n2. **GPT4All** \u2014 \u044d\u0442\u043e \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u044b\u0439, \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0449\u0438\u0439 \u0447\u0430\u0442-\u0431\u043e\u0442, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044e\u0449\u0438\u0439 \u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c. \u041d\u0438\u043a\u0430\u043a\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430 \u0438\u043b\u0438 \u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0430 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f. \u0412 \u043d\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a GPT4All Falcon, Wizard \u0438 \u0442. \u0434. \u0410 \u0442\u0430\u043a\u0436\u0435 \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u0442\u043e\u0434 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 **GPT4All embeddings** \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e **LangChain**. https://python.langchain.com/docs/integrations/text_embedding/gpt4all\n\n\n\n3. **Llama-cpp (llama-cpp-python)** - \u043c\u0435\u0442\u043e\u0434 **LlamaCppEmbeddings** \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e **LangChain** \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 \u043a\u0430\u0447\u0435\u0442\u0441\u0442\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u043d\u0438\u044f. https://python.langchain.com/docs/integrations/text_embedding/llamacpp\n## \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439\n!pip install openai==0.27.8 chromadb==0.4.5 langchain==0.0.262 pydantic==2.1.1 faiss-cpu==1.7.4 tiktoken==0.4.0 nltk==3.8.1 gpt4all==1.0.8 InstructorEmbedding==1.0.1\n\n!pip install -U deep-translator==1.11.4\n\n!pip install --force --no-deps git+https://github.com/UKPLab/sentence-transformers.git\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.77 --force-reinstall --upgrade --no-cache-dir\n\n## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n!git lfs install\n\n!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n!wget https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit/resolve/main/pytorch_model.bin\n!wget https://huggingface.co/TheBloke/Llama-2-7B-GGML/resolve/main/llama-2-7b.ggmlv3.q4_1.bin\n!wget https://huggingface.co/TheBloke/Llama-2-7B-GGML/resolve/main/llama-2-7b.ggmlv3.q8_0.bin\n!git lfs install\n\n!git clone https://huggingface.co/TheBloke/Llama-2-7B-fp16\n!git lfs install\n\n!git clone https://huggingface.co/intfloat/multilingual-e5-large\n\n## \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c\nimport pandas as pd\n\n\n\ndf_qw = pd.read_csv(\"/content/\u0411\u0430\u0437\u0430 \u0437\u043d\u0430\u043d\u0438\u0439 (\u0432\u043e\u043f\u0440\u043e\u0441_\u043e\u0442\u0432\u0435\u0442) - \u041b\u0438\u0441\u04421.csv\")\n\ndf_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values\n\n### HugginFace Embeddings\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nnltk.download('punkt')\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local\"\n\nmodels = [\n\n          \"all-MiniLM-L6-v2\",\n\n          \"all-mpnet-base-v2\",\n\n          \"multilingual-e5-large\",\n\n          \"Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit\",\n\n          \"Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\",\n\n          ]\n\n\n\n\n\nloader = TextLoader(\"/content/0 \u041a\u043e\u043f\u0438\u0440\u0430\u0439\u0442\u0438\u043d\u0433. \u0422\u0435\u043a\u0441\u0442 \u0434\u043b\u044f ChatGPT.txt\")\n\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n\ndocs = text_splitter.split_documents(documents)\n\n\n\nfor model in models:\n\n  history = []\n\n  embeddings = SentenceTransformerEmbeddings(model_name=model)\n\n  db = FAISS.from_documents(docs, embeddings)\n\n  for query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n    sim_docs = db.similarity_search_with_score(query)\n\n    doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\u041e\u0442\u0440\u044b\u0432\u043e\u043a \u2116{i+1} \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n    history.append([str(query), str(doc)])\n\n  pd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, os.path.split(model)[1]+'.csv'), index=False)\n\n\n### OpenAI Emmbeddings\nimport os\n\nimport getpass\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nnltk.download('punkt')\n\n\n\n\n\nloader = TextLoader(\"/content/0 \u041a\u043e\u043f\u0438\u0440\u0430\u0439\u0442\u0438\u043d\u0433. \u0422\u0435\u043a\u0441\u0442 \u0434\u043b\u044f ChatGPT.txt\")\n\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n\ndocs = text_splitter.split_documents(documents)\n\ndb = FAISS.from_documents(docs, OpenAIEmbeddings())\n\nhistory = []\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(query)\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\u041e\u0442\u0440\u044b\u0432\u043e\u043a \u2116{i+1} \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([str(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, 'openai'+'.csv'), index=False)", "metadata": {"subid": 0, "total": 3, "source": "https://colab.research.google.com/drive/1tc8-0FVeu0dNrHu_vzAvLeWlBa0aeEAB"}}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n### Llama2 Embeddings\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nnltk.download('punkt')\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local\"\n\nmodels = [\n\n    \"llama-2-7b.ggmlv3.q4_1.bin\",\n\n    \"llama-2-7b.ggmlv3.q8_0.bin\"\n\n]\n\n\n\nloader = TextLoader(\"/content/0 \u041a\u043e\u043f\u0438\u0440\u0430\u0439\u0442\u0438\u043d\u0433. \u0422\u0435\u043a\u0441\u0442 \u0434\u043b\u044f ChatGPT.txt\")\n\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n\ndocs = text_splitter.split_documents(documents)\nmodel = models[0]\n\n\n\nhistory = []\n\nllama_embeddings = LlamaCppEmbeddings(model_path=model, n_gpu_layers=400, n_batch=512, n_ctx=2048)\n\ndb = FAISS.from_documents(docs, llama_embeddings)\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(query)\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\u041e\u0442\u0440\u044b\u0432\u043e\u043a \u2116{i+1} \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([str(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, model+'.csv'), index=False)\n\nmodel = models[1]\n\n\n\nhistory = []\n\nllama_embeddings = LlamaCppEmbeddings(model_path=model, n_gpu_layers=200, n_batch=512, n_ctx=2048)\n\ndb = FAISS.from_documents(docs, llama_embeddings)\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(query)\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\u041e\u0442\u0440\u044b\u0432\u043e\u043a \u2116{i+1} \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([str(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, model+'.csv'), index=False)\n\n\n## \u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 c GPT4 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c\n### \u041e\u0446\u0435\u043d\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nimport os\n\nimport getpass\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\nimport openai\n\n\n\ndef get_answer(system, instruction, topic, message_content, temp=0.5):\n\n\n\n    messages = [\n\n        {\"role\": \"system\", \"content\": system},\n\n        {\"role\": \"user\", \"content\": f\"{instruction}.\\n\\n\u0412\u043e\u043f\u0440\u043e\u0441:\\n{topic}\\n\\n\u0424\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u044b \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\\n\"\n\n                                        f\"{message_content}.\"}\n\n    ]\n\n\n\n    completion = openai.ChatCompletion.create(\n\n        model=\"gpt-4\",\n\n        messages=messages,\n\n        temperature=temp\n\n    )\n\n\n\n    return completion.choices[0].message.content\nsystem = \"\"\"\n\n\u0422\u0432\u043e\u044f \u0437\u0430\u0434\u0430\u0447\u0430: \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u044c (\u0431\u043b\u0438\u0437\u043e\u0441\u0442\u044c) \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u043e\u043f\u0440\u043e\u0441\u0430 \u0438 \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0445 \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u043e\u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0438\u0437 \u0431\u0430\u0437\u044b \u0437\u043d\u0430\u043d\u0438\u0439 \u043d\u0430 \u043f\u043e\u043b\u043d\u043e\u0442\u0443 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e\u0439 \u0434\u043b\u044f \u043e\u0442\u0432\u0435\u0442\u0430.\n\n\"\"\"\n\n\n\ninstruction = \"\"\"\n\n\u041e\u0446\u0435\u043d\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0438\u0441\u043a\u043e\u0432\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u043e 10 \u0431\u0430\u043b\u044c\u043d\u043e\u0439 \u0448\u043a\u0430\u043b\u0435 \u0433\u0434\u0435 10 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u0430\u044f \u0434\u043b\u044f \u043e\u0442\u0432\u0435\u0442\u0430 \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441 \u0438 0 \u0435\u0441\u043b\u0438 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u0438 \u043d\u0435\u0442. \u0424\u043e\u0440\u043c\u0430 \u043e\u0442\u0432\u0435\u0442\u0430 - \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u043e\u0442 0 \u0434\u043e 10.\n\n\"\"\"\nimport pandas as pd\n\nimport time\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local\"\n\nfiles = os.listdir(project_path)\n\n\n\n\n\nscore = [[] for i in range(18)]\n\nfor count, file in enumerate(files):\n\n  df = pd.read_csv(os.path.join(project_path, file))\n\n\n\n  for i, (topic, message_content) in enumerate(df.values):\n\n    if count == 0:\n\n      score[i].append(topic)\n\n    answer = get_answer(system, instruction, topic, message_content, temp=0.5)\n\n    score[i].append(answer)\n\n    time.sleep(10)\n\n\n\npd.DataFrame(score, columns=['\u0412\u043e\u043f\u0440\u043e\u0441']+files).to_csv(os.path.join(project_path, 'score_result'+'.csv'), index=False)\n\n### \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u0432 \u043e\u0434\u043d\u0443 \u0442\u0430\u0431\u043b\u0438\u0446\u0443\nimport pandas as pd\n\nimport time\n\nimport os\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local\"\n\nfiles = os.listdir(project_path)\n\n\n\ndf_all = pd.DataFrame(columns=['\u0412\u043e\u043f\u0440\u043e\u0441']+files)\n\nfor count, file in enumerate(files):\n\n  if file not in ['score_result.csv', 'score_result.xlsx', \".ipynb_checkpoints\"]:\n\n    df = pd.read_csv(os.path.join(project_path, file))\n\n    if count == 0:\n\n      df_all['\u0412\u043e\u043f\u0440\u043e\u0441'] = df['\u0412\u043e\u043f\u0440\u043e\u0441']\n\n      df_all[file] = df['\u041e\u0442\u0440\u044b\u0432\u043a\u0438']\n\n    else:\n\n      df_all[file] = df['\u041e\u0442\u0440\u044b\u0432\u043a\u0438']\n\n\n\ndf_all.to_csv(os.path.join(project_path, 'all_text_result'+'.csv'), index=False)\n\n## \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c\n!git lfs install\n\n!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit\n!wget https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit/resolve/main/pytorch_model.bin\nimport pandas as pd\n\n\n\ndf_qw = pd.read_csv(\"/content/\u0411\u0430\u0437\u0430 \u0437\u043d\u0430\u043d\u0438\u0439 (\u0432\u043e\u043f\u0440\u043e\u0441_\u043e\u0442\u0432\u0435\u0442) - \u041b\u0438\u0441\u04421.csv\")\n\ndf_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values\nfrom langchain.llms import LlamaCpp\n\nfrom langchain import PromptTemplate, LLMChain\n\nfrom langchain.callbacks.manager import CallbackManager\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nloader = TextLoader(\"/content/0 \u041a\u043e\u043f\u0438\u0440\u0430\u0439\u0442\u0438\u043d\u0433. \u0422\u0435\u043a\u0441\u0442 \u0434\u043b\u044f ChatGPT.txt\")\n\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n\ndocs = text_splitter.split_documents(documents)\ndocs[100].page_content\nimport tqdm\n\nfrom deep_translator import GoogleTranslator\n\n\n\ndocs_translate = []\n\nfor doc in tqdm.notebook.tqdm(docs):\n\n  # docs_translate.append(Document(page_content=llm_chain.run(doc.page_content)))\n\n  docs_translate.append(Document(page_content=GoogleTranslator(source='auto', target='en').translate(doc.page_content)))\ndocs_translate[100]\n\n\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0447\u0430\u043d\u043a\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u0435\u0440\u0435\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0435 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u0432 pkl \u0444\u0430\u0439\u043b\nimport pickle\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\n\n\nwith open(os.path.join(project_path, 'docs_translate.pkl'), 'wb') as f:\n\n  pickle.dump(docs_translate, f, protocol = None, fix_imports = True)\n\n### HugginFace Embeddings\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nimport pickle\n\nfrom deep_translator import GoogleTranslator\n\nnltk.download('punkt')\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\nmodels = [\n\n          \"all-MiniLM-L6-v2\",\n\n          \"all-mpnet-base-v2\",\n\n          \"intfloat/multilingual-e5-large\",\n\n          \"SGPT-1.3B-weightedmean-msmarco-specb-bitfit\",\n\n          \"SGPT-5.8B-weightedmean-msmarco-specb-bitfit\",\n\n          ]\n\n\n\nwith open(os.path.join(project_path, 'docs_translate.pkl'), 'rb') as f:\n\n  docs_translate = pickle.load(f)\n\n\n\nfor model in models:\n\n  history = []\n\n  embeddings = SentenceTransformerEmbeddings(model_name=model)\n\n  db = FAISS.from_documents(docs_translate, embeddings) # GPT4AllEmbeddings() \u0432\u044b\u043b\u0435\u0442\u0430\u0435\u0442 \u043f\u043e \u043f\u0430\u043c\u044f\u0442\u0438\n\n  for query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n    sim_docs = db.similarity_search_with_score(GoogleTranslator(source='auto', target='en').translate(query))\n\n    doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Fragment \u2116{i+1} document:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n    history.append([GoogleTranslator(source='auto', target='en').translate(query), str(doc)])\n\n  pd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, os.path.split(model)[1]+'.csv'), index=False)\n\n\n### OpeAI Emmbeddings\nimport os\n\nimport getpass\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nimport pickle\n\nfrom deep_translator import GoogleTranslator\n\nnltk.download('punkt')\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\n\n\nwith open(os.path.join(project_path, 'docs_translate.pkl'), 'rb') as f:\n\n  docs_translate = pickle.load(f)\n\n\n\ndb = FAISS.from_documents(docs_translate, OpenAIEmbeddings())\n\nhistory = []\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(GoogleTranslator(source='auto', target='en').translate(query))\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Fragment \u2116{i+1} document:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([GoogleTranslator(source='auto', target='en').translate(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, 'openai'+'.csv'), index=False)", "metadata": {"subid": 1, "total": 3, "source": "https://colab.research.google.com/drive/1tc8-0FVeu0dNrHu_vzAvLeWlBa0aeEAB"}}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "\n### Llama2 Embeddings\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n\nfrom langchain.document_loaders import TextLoader\n\nfrom langchain.embeddings import LlamaCppEmbeddings\n\nfrom langchain.embeddings import GPT4AllEmbeddings\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nfrom langchain.vectorstores import FAISS\n\nfrom langchain.llms import OpenAI\n\nfrom langchain.docstore.document import Document\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n\nimport nltk\n\nimport os\n\nimport re\n\nimport pickle\n\nfrom deep_translator import GoogleTranslator\n\nnltk.download('punkt')\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\nmodels = [\n\n    \"llama-2-7b.ggmlv3.q4_1.bin\",\n\n    \"llama-2-7b.ggmlv3.q8_0.bin\"\n\n]\n\n\n\n\n\nwith open(os.path.join(project_path, 'docs_translate.pkl'), 'rb') as f:\n\n  docs_translate = pickle.load(f)\nmodel = models[0]\n\n\n\nhistory = []\n\nllama_embeddings = LlamaCppEmbeddings(model_path=model, n_gpu_layers=400, n_batch=512, n_ctx=2048)\n\ndb = FAISS.from_documents(docs_translate, llama_embeddings)\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(GoogleTranslator(source='auto', target='en').translate(query))\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Fragment \u2116{i+1} document:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([GoogleTranslator(source='auto', target='en').translate(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, model+'.csv'), index=False)\n\nmodel = models[1]\n\n\n\nhistory = []\n\nllama_embeddings = LlamaCppEmbeddings(model_path=model, n_gpu_layers=200, n_batch=512, n_ctx=2048)\n\ndb = FAISS.from_documents(docs_translate, llama_embeddings)\n\nfor query in df_qw['\u0412\u043e\u043f\u0440\u043e\u0441'].values:\n\n  sim_docs = db.similarity_search_with_score(GoogleTranslator(source='auto', target='en').translate(query))\n\n  doc = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Fragment \u2116{i+1} document:\\n' + doc[0].page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n\n  history.append([GoogleTranslator(source='auto', target='en').translate(query), str(doc)])\n\npd.DataFrame(history, columns=['\u0412\u043e\u043f\u0440\u043e\u0441', '\u041e\u0442\u0440\u044b\u0432\u043a\u0438']).to_csv(os.path.join(project_path, model+'.csv'), index=False)\n\n\n## \u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 c GPT4 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c\n### \u041e\u0446\u0435\u043d\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nimport os\n\nimport getpass\n\n\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\nimport openai\n\n\n\ndef get_answer(system, instruction, topic, message_content, temp=0.5):\n\n\n\n    messages = [\n\n        {\"role\": \"system\", \"content\": system},\n\n        {\"role\": \"user\", \"content\": f\"{instruction}.\\n\\nQuestion:\\n{topic}\\n\\nDocument fragments\\n\"\n\n                                        f\"{message_content}.\"}\n\n    ]\n\n\n\n    completion = openai.ChatCompletion.create(\n\n        model=\"gpt-4\",\n\n        messages=messages,\n\n        temperature=temp\n\n    )\n\n\n\n    return completion.choices[0].message.content\nsystem = \"\"\"\n\nYour task is to assess the similarity (proximity) of the question asked and the fragments of documents found from the knowledge base for completeness of information sufficient to answer.\n\n\"\"\"\n\n\n\ninstruction = \"\"\"\n\nEvaluate the quality of the search engine on a 10-point scale where 10 is the maximum similarity sufficient to answer the question and 0 if there is no similarity. The answer form is a number from 0 to 10.\n\n\"\"\"\nimport pandas as pd\n\nimport time\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\nfiles = os.listdir(project_path)\n\n\n\n\n\nscore = [[] for i in range(18)]\n\nfor count, file in enumerate(files):\n\n  if file not in ['score_result_en.csv', 'score_result.xlsx', '.ipynb_checkpoints']:\n\n    df = pd.read_csv(os.path.join(project_path, file))\n\n\n\n    for i, (topic, message_content) in enumerate(df.values):\n\n      if count == 0:\n\n        score[i].append(topic)\n\n      answer = get_answer(system, instruction, topic, message_content, temp=0.5)\n\n      score[i].append(answer)\n\n      time.sleep(10)\n\n\n\npd.DataFrame(score, columns=['\u0412\u043e\u043f\u0440\u043e\u0441']+files).to_csv(os.path.join(project_path, 'score_result_en'+'.csv'), index=False)\n\n### \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c \u0432 \u043e\u0434\u043d\u0443 \u0442\u0430\u0431\u043b\u0438\u0446\u0443\nimport pandas as pd\n\nimport time\n\nimport os\n\n\n\nproject_path = \"/content/drive/MyDrive/\u041f\u0440\u043e\u0435\u043a\u0442\u044b/Embeddings_local_en\"\n\nfiles = os.listdir(project_path)\n\n\n\ndf_all = pd.DataFrame(columns=['\u0412\u043e\u043f\u0440\u043e\u0441']+files)\n\nfor count, file in enumerate(files):\n\n  if file not in ['score_result_en.csv', 'score_result.xlsx', '.ipynb_checkpoints']:\n\n    df = pd.read_csv(os.path.join(project_path, file))\n\n    if count == 0:\n\n      df_all['\u0412\u043e\u043f\u0440\u043e\u0441'] = df['\u0412\u043e\u043f\u0440\u043e\u0441']\n\n      df_all[file] = df['\u041e\u0442\u0440\u044b\u0432\u043a\u0438']\n\n    else:\n\n      df_all[file] = df['\u041e\u0442\u0440\u044b\u0432\u043a\u0438']\n\n\n\ndf_all.to_csv(os.path.join(project_path, 'all_text_result'+'.csv'), index=False)\n\n# \u0412\u044b\u0432\u043e\u0434\u044b\n**\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0446\u0435\u043d\u043e\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u043d\u0430 \u043d\u0430\u0448\u0435\u0439 \u0431\u0430\u0437\u0435 \u043f\u043e \u0448\u043a\u0430\u043b\u0435 \u043e\u0442 0 \u0434\u043e 10**\n\u041e\u0446\u0435\u043d\u043a\u0438 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u0438 \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0445 \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u043e\u0432 \u0438 \u0432\u043e\u043f\u0440\u043e\u0441\u0430 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435\n\u041e\u0446\u0435\u043d\u043a\u0438 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u0438 \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0445 \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u043e\u0432 \u0438 \u0432\u043e\u043f\u0440\u043e\u0441\u0430 \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435", "metadata": {"subid": 2, "total": 3, "source": "https://colab.research.google.com/drive/1tc8-0FVeu0dNrHu_vzAvLeWlBa0aeEAB"}}}]