{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8273703-ab6b-4d8f-9520-0cda68d1c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a50bdda-69a7-42e1-9b00-49eb9cd16bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'TOKEN' # fake token - we actually wom't call API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2333dff6-5b79-47d6-8d1d-d80ece289a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = './lessons_notebook'\n",
    "db = FAISS.load_local(\n",
    "    db_path, \n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91f94a5-c0e0-4a2c-8eea-7677a70dece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 167/167 [00:00<00:00, 154829.52it/s]\n"
     ]
    }
   ],
   "source": [
    "vectores = {}\n",
    "for k, v in tqdm(db.index_to_docstore_id.items()):\n",
    "  vectores[v] = db.index.reconstruct(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a1bbe5-3a9a-41a0-8ff6-3e3ee1f57ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e6619753-2a31-5607-b2ae-4bfa1c0d06f3',\n",
       " '88de087f-0fb6-54d8-bd52-1490d7f897ca',\n",
       " 'f080faeb-0ef2-58aa-a73f-f41fb3271ba1',\n",
       " '995d0b54-8631-5186-9891-d80d36ac864a',\n",
       " '08740ad7-f998-5461-b054-50411fba8c35',\n",
       " '7e72e671-8061-5cb4-bf30-bc5ef3662c78',\n",
       " 'dab592eb-3ade-567d-9ed0-98375d56e640',\n",
       " 'f3e7a84a-39b6-5cf8-93da-6d76150fa7ef',\n",
       " '7d8916db-0b3a-5f7a-a0e6-d3e7bc05a45a',\n",
       " 'd862faf3-f273-5057-91b7-be2411d0fb9f']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectores.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "197af28c-4dbe-4aab-ad72-a80ad5b6d7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'subid': 0, 'total': 1, 'source': 'https://colab.research.google.com/drive/1N1t5yPSnoLhKMvZj4wXNSPOypzdsNHl_?usp=drive_link'}, page_content='\\n!pip install -q tiktoken==0.7.0 openai langchain langchain-openai langchain-text-splitters >/dev/null\\n\\n!pip install langchain_community >/dev/null\\n\\n!pip install --upgrade --quiet  langchain-qdrant >/dev/null\\nimport openai\\n\\nfrom openai import OpenAI\\n\\nimport tiktoken\\n\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\\n\\nfrom langchain.docstore.document import Document\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nimport os\\n\\nfrom google.colab import drive\\n\\ndrive.mount(\\'/content/drive\\', force_remount=True)\\n\\nimport re\\n\\nimport json\\n\\n\\n\\nfrom langchain_qdrant import Qdrant\\nfrom google.colab import userdata\\n\\nkey = userdata.get(\\'OpenAI\\') # здесь можно заменить название секрета колаба на свое (для ключа ОпенЭйАй)\\n\\nos.environ[\"OPENAI_API_KEY\"] = key\\n\\nclient = OpenAI()\\nMODEL = \\'gpt-3.5-turbo-0125\\'\\ngoogle_folder_path = \"/content/drive/MyDrive/Neural_University/GPT_Practic/24-07-09\"\\n\\nf_name = \\'Инструменты\\'\\n\\nbd_full_name = os.path.join(google_folder_path, f\\'{f_name}.txt\\')\\n\\nwith open(bd_full_name, \\'r\\') as f:\\n\\n    markdawn = f.read()\\n# @title Функции\\n\\n# Функция подсчета токенов\\n\\ndef num_tokens(str_, model=MODEL):\\n\\n    try:\\n\\n        encoding = tiktoken.encoding_for_model(model)\\n\\n    except KeyError:\\n\\n        encoding = tiktoken.get_encoding(\\'cl100k_base\\')\\n\\n    num_tokens = len(encoding.encode(str_))\\n\\n    return num_tokens\\n\\n\\n\\n#@title Разбор текстового файла (функция)\\n\\ndef parse_text(text):\\n\\n    # Функция разбирает текст на чанки по разделителю \\'---...---\\'\\n\\n    # Текст содержит словари с метадатой - их разбираем при помощи json\\n\\n    # Есть нюансы по замене одинарных кавычек на двойные в ключах и значениях для json-словарей\\n\\n    # распознанные чанки превращаем в список документов лангчейн\\n\\n    #  в метаданных меняем названия разделов с H1 на group и т.д.\\n\\n\\n\\n    chunks = text.split(\\'----------------------------------------------------------------\\')\\n\\n    source_chunks = []\\n\\n    # Регулярное выражение для поиска JSON словаря в начале каждого чанка\\n\\n    json_pattern = re.compile(r\"\\\\{.*?\\\\}\", re.DOTALL)\\n\\n    for chunk in chunks:\\n\\n        chunk = chunk.strip()\\n\\n        if not chunk:\\n\\n            continue\\n\\n        # Поиск словаря\\n\\n        json_match = json_pattern.search(chunk)\\n\\n        if json_match:\\n\\n            json_str = json_match.group()\\n\\n            # Заменим двойные кавычки на временный маркер\\n\\n            json_str = json_str.replace(\\'\"\\', \\'TEMP_DOUBLE_QUOTE\\')\\n\\n            # Заменим одинарные кавычки на двойные без экранирования, так json распознает словарь\\n\\n            json_str = json_str.replace(\"\\'\", \\'\"\\')\\n\\n\\n\\n            # Вернем внутренние двойные кавычки в значение полей словаря с экранированием (для json)\\n\\n            json_str = json_str.replace(\\'TEMP_DOUBLE_QUOTE\\', \\'\\\\\\\\\"\\')\\n\\n            metadata = {}\\n\\n            try:\\n\\n                json_dict = json.loads(json_str)\\n\\n                if \\'H1\\' in json_dict:\\n\\n                    metadata[\\'group\\'] = json_dict[\\'H1\\']\\n\\n                if \\'H2\\' in json_dict:\\n\\n                    metadata[\\'subgroup\\'] = json_dict[\\'H2\\']\\n\\n                for i in range(3, 7):\\n\\n                    if f\\'H{i}\\' in json_dict:\\n\\n                        if json_dict[f\\'H{i}\\'] == \\'Товары\\':\\n\\n                            metadata[\\'item\\'] = json_dict[f\\'H{i}\\']\\n\\n                        else:\\n\\n                            metadata[f\\'H{i}\\'] = json_dict[f\\'H{i}\\']\\n\\n\\n\\n            except json.JSONDecodeError as e:\\n\\n                print(f\"Ошибка декодирования JSON: {e}\")\\n\\n                print(f\"Строка JSON: {json_str}\")\\n\\n                continue\\n\\n\\n\\n            # Уберем JSON словарь из начала чанка\\n\\n            page_content = chunk[json_match.end():].strip()\\n\\n\\n\\n            # Добавление результата в список\\n\\n            source_chunks.append(Document(page_content = page_content, metadata = metadata))\\n\\n\\n\\n    return source_chunks\\n\\n\\n\\n#@title Сплиттер (функция)\\n\\ndef split_doc( text_, max_tokens=100, markdown_max_level=4):\\n\\n  # Функция разбивает text_ на чанки длины max_tokens,\\n\\n  # Если текст меньше  max_tokens, то оставляет как есть,\\n\\n  # иначе сначала разбивает MarkdownHeaderTextSplitter до уровня markdown_max_level,\\n\\n  # полученные чанки (если они больше max_tokens) разбивает RecursiveCharacterTextSplitter,\\n\\n  # в тело чанков дублирует заголовки в их иерархии\\n\\n  # возвращает список документов лангчейн:\\n\\n  #   в page_content текстовые чанки,\\n\\n  #   в meta_data - заголовки маркдаун, имя файла\\n\\n\\n\\n    chunk_list = []\\n\\n    #headers_to_split_on = [(f\"{\\'#\\' * i}\", f\"H{i}\") for i in range(1, markdown_max_level+1)]\\n\\n    headers_to_split_on = [\\n\\n        (\\'#\\',\\'group\\'),\\n\\n        (\\'##\\',\\'subgroup\\'),\\n\\n        (\\'###\\',\\'subsubgroup\\'),\\n\\n        (\\'####\\',\\'items\\')\\n\\n    ]\\n\\n    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\\n\\n    chunks = markdown_splitter.split_text(text_)\\n\\n\\n\\n    for chunk in chunks:\\n\\n            headers = \\'\\'\\n\\n            ch_txt = chunk.page_content\\n\\n\\n\\n\\n\\n            for header_key in list(chunk.metadata):\\n\\n                headers += f\\'\\\\n{header_key}: {chunk.metadata[header_key]}\\'\\n\\n            pcont = f\\'{headers}\\\\n{ch_txt}\\'\\n\\n            chunk_len = num_tokens(pcont)\\n\\n\\n\\n            if chunk_len > max_tokens:\\n\\n                r_splitter = RecursiveCharacterTextSplitter(separators = [\"\\\\n\\\\n\", \"\\\\n\"],\\n\\n                                                            chunk_size = max_tokens - num_tokens(headers),\\n\\n                                                            chunk_overlap = 0,\\n\\n                                                            length_function = lambda x: num_tokens(x))\\n\\n                parts = r_splitter.split_text(pcont)\\n\\n                for k, part in enumerate(parts):\\n\\n                    if k == 0: # в первом чанке есть заголовки (от маркдаун-чанка)\\n\\n                        ppcont = part\\n\\n                    else:\\n\\n                        ppcont = f\\'Продолжение {k}{headers}\\\\n{part}\\'\\n\\n                    metadata = chunk.metadata\\n\\n                    metadata[\\'tokens\\'] =  num_tokens(ppcont)\\n\\n                    metadata[\\'part_num\\'] =  k+1\\n\\n                    chunk_list.append(Document(page_content=ppcont, metadata=metadata))\\n\\n            else:\\n\\n                metadata = chunk.metadata\\n\\n                metadata[\\'tokens\\'] =  chunk_len\\n\\n                chunk_list.append(Document(page_content=pcont, metadata=metadata))\\n\\n\\n\\n    return chunk_list\\nparsed_chunks = parse_text(markdawn)\\n\\nprint(len(parsed_chunks))\\n# Вывод результатов\\n\\nfor chunk in range(4,7):\\n\\n    print(f\\'Чанк {chunk}: {parsed_chunks[chunk].metadata}\\\\n{parsed_chunks[chunk].page_content}\\')\\n\\n    print(\\'______________\\')\\nimport matplotlib.pyplot as plt\\n\\n# Подсчет токенов для каждого фрагмента и построение графика\\n\\nfragment_token_counts = [num_tokens(fragment.page_content, \"cl100k_base\") for fragment in parsed_chunks]\\n\\nplt.hist(fragment_token_counts, bins=50, alpha=0.5, label=\\'Fragments\\')\\n\\nplt.title(\\'Распределение длин чанков в токенах\\')\\n\\nplt.xlabel(\\'Token Count\\')\\n\\nplt.ylabel(\\'Frequency\\')\\n\\nplt.show()\\n#@title С фильтрацией\\n\\nembeddings = OpenAIEmbeddings()\\nqdrant = Qdrant.from_documents(\\n\\n    parsed_chunks,\\n\\n    embeddings,\\n\\n    location=\":memory:\",  # Local mode with in-memory storage only\\n\\n    collection_name=\"Инструменты\",\\n\\n)\\nquery = \"Мусор\"\\n\\nfound_docs = qdrant.similarity_search(query, k=3)\\nfor doc in found_docs:\\n\\n  print(\\'__________\\')\\n\\n  print(doc.page_content)\\n\\n# Будем фильтровать\\n\\nfrom qdrant_client.http import models as rest\\n\\n\\n\\nfilter_condition = rest.FieldCondition(\\n\\n    key=\\'metadata.item\\',   # здесь указываем не только ключ в метаданных но и сами метаданные (как словарь называется, т.к. есть варианты)\\n\\n    match=rest.MatchValue(value=\\'Товары\\')\\n\\n)\\n\\n\\n\\nquery_filter = rest.Filter(must=[filter_condition])\\n\\n\\n\\nfound_docs = qdrant.similarity_search(query, k=3, filter=query_filter)\\n\\nlen(found_docs)\\nfor doc in found_docs:\\n\\n  print(\\'__________\\')\\n\\n  if doc: print(doc.page_content)\\noutput = qdrant.similarity_search(query, k=3, filter={\"metadata\": {\"item\": \\'Товары\\'}})\\nfor doc in found_docs:\\n\\n  print(\\'__________\\')\\n\\n  if doc: print(doc.page_content)\\n\\nhttps://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/tests/integration_tests/test_similarity_search.py\\n\\n\\n\\n\\n    \"\"\"Test end to end construction and search.\"\"\"\\n\\n    texts = [\"foo\", \"bar\", \"baz\"]\\n\\n    metadatas = [\\n\\n        {\"page\": i, \"metadata\": {\"page\": i + 1, \"pages\": [i + 2, -1]}}\\n\\n        for i in range(len(texts))\\n\\n    ]\\n\\n    docsearch = Qdrant.from_texts(\\n\\n        texts,\\n\\n        ConsistentFakeEmbeddings(),\\n\\n        metadatas=metadatas,\\n\\n        location=\":memory:\",\\n\\n        batch_size=batch_size,\\n\\n        vector_name=vector_name,\\n\\n    )\\n\\n\\n\\n    output = docsearch.similarity_search(\\n\\n        \"foo\", k=1, filter={\"page\": 1, \"metadata\": {\"page\": 2, \"pages\": [3]}}\\n\\n    )\\n\\n\\n\\n    assert_documents_equals(\\n\\n        actual=output,\\n\\n        expected=[\\n\\n            Document(\\n\\n                page_content=\"bar\",\\n\\n                metadata={\"page\": 1, \"metadata\": {\"page\": 2, \"pages\": [3, -1]}},\\n\\n            )\\n\\n        ],\\n\\n    )')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document\n",
    "key = list(vectores.keys())[0]\n",
    "doc = db.docstore._dict[key]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51ba94bc-bf88-4257-950b-5b103ad93e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subid': 0,\n",
       " 'total': 1,\n",
       " 'source': 'https://colab.research.google.com/drive/1N1t5yPSnoLhKMvZj4wXNSPOypzdsNHl_?usp=drive_link'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba9fb7-8707-4fc5-a12f-09c5fc0800f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
